{"cells":[{"cell_type":"markdown","metadata":{"id":"jqDHq_AEjRZ1"},"source":["## 1. Представление и предобработка текстовых данных "]},{"cell_type":"markdown","metadata":{"id":"vaki7efDpmXo"},"source":["1.1 Операции по предобработке:\n","* токенизация\n","* стемминг / лемматизация\n","* удаление стоп-слов\n","* удаление пунктуации\n","* приведение к нижнему регистру\n","* любые другие операции над текстом"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nHRy4jpYphEr"},"outputs":[],"source":["from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.stem.snowball import SnowballStemmer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lMMzGhq0ikz1"},"outputs":[],"source":["text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"]},{"cell_type":"markdown","metadata":{"id":"bUhfertRtXE5"},"source":["Реализовать функцию `preprocess_text(text: str) -> str`, которая:\n","* приводит строку к нижнему регистру\n","* заменяет все символы, кроме a-z, A-Z и знаков .,!? на пробел\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OtNi49QJhWAj","outputId":"30d2ba22-3a1e-40b1-8a63-337910b96ce6"},"outputs":[{"data":{"text/plain":["'select your preferences and run the install command. stable represents the most currently tested and supported version of pytorch. note that libtorch is only available for c  '"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import re\n","# reg = re.compile(r\"[^a-z A-Z.]\")\n","def preprocess_text(text: str) -> str:\n","    return re.sub(r\"[^a-z A-Z\\.]\", \" \", text.lower())\n","preprocess_text(text)"]},{"cell_type":"markdown","metadata":{"id":"Z2Dt1ssIqckC"},"source":["1.2 Представление текстовых данных при помощи бинарного кодирования\n","\n","\n","Представить первое предложение из `text` в виде тензора `sentence_t`: `sentence_t[i] == 1`, если __слово__ с индексом `i` присуствует в предложении."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxBjAO71hWAk"},"outputs":[],"source":["import nltk\n","from pprint import pprint\n","nltk.download('punkt') # ноебходимо скачать для полноценной работы"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y85ewbmrhWAl","outputId":"bbe1867e-9835-4cc9-a9f6-e36f5fbc560b"},"outputs":[{"data":{"text/plain":["'Select your preferences and run the install command'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["text.split(\".\")[0] # первое предложение "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25Sdv9QYhWAm","outputId":"23f45e42-ad93-4a91-d035-c3b3d5eb64f8"},"outputs":[{"data":{"text/plain":["[0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["tokens = list(set(word_tokenize(preprocess_text(text))))\n","def get_embed(text,tokens):\n","    text = preprocess_text(text)\n","    x = [0]* len(tokens)\n","    for word in word_tokenize(text):\n","        x[tokens.index(word)] = 1\n","    return x\n","\n","\n","    \n","get_embed(text.split(\".\")[0],tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7tbN4aR2hWAn","outputId":"0bc10239-11d6-421e-f6b6-b4cec90c6b84"},"outputs":[{"name":"stdout","output_type":"stream","text":["['pytorch', 'of', 'the', 'c', '.', 'and', 'run', 'libtorch', 'most', 'stable', 'only', 'your', 'currently', 'supported', 'available', 'is', 'install', 'select', 'represents', 'tested', 'note', 'command', 'that', 'preferences', 'for', 'version']\n"]}],"source":["print(tokens)"]},{"cell_type":"markdown","metadata":{"id":"P2Nz_zcgw3N4"},"source":["## 2. Классификация фамилий по национальности\n","\n","Датасет: https://disk.yandex.ru/d/owHew8hzPc7X9Q?w=1\n","\n","2.1 Считать файл `surnames/surnames.csv`. \n","\n","2.2 Закодировать национальности числами, начиная с 0.\n","\n","2.3 Разбить датасет на обучающую и тестовую выборку\n","\n","2.4 Реализовать класс `Vocab` (токен = __символ__)\n","\n","2.5 Реализовать класс `SurnamesDataset`\n","\n","2.6. Обучить классификатор.\n","\n","2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: прогнать несколько фамилий студентов группы через модели и проверить результат. Для каждой фамилии выводить 3 наиболее вероятных предсказания."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HH6azjKEhWAo"},"outputs":[],"source":["import pandas as pd\n","from sklearn import preprocessing,model_selection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10EIqtcQhWAo"},"outputs":[],"source":["le = preprocessing.LabelEncoder() # энкодер - преобразует название класса  в номер"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E03v1p1dhWAo","outputId":"c1200fa4-0b88-4c50-8112-43f10faeea0a"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>surname</th>\n","      <th>nationality</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>woodford</td>\n","      <td>English</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>coté</td>\n","      <td>French</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>kore</td>\n","      <td>English</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>koury</td>\n","      <td>Arabic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>lebzak</td>\n","      <td>Russian</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>10975</th>\n","      <td>quraishi</td>\n","      <td>Arabic</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>10976</th>\n","      <td>innalls</td>\n","      <td>English</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>10977</th>\n","      <td>król</td>\n","      <td>Polish</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>10978</th>\n","      <td>purvis</td>\n","      <td>English</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>10979</th>\n","      <td>messerli</td>\n","      <td>German</td>\n","      <td>6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10980 rows × 3 columns</p>\n","</div>"],"text/plain":["        surname nationality  label\n","0      woodford     English      4\n","1          coté      French      5\n","2          kore     English      4\n","3         koury      Arabic      0\n","4        lebzak     Russian     14\n","...         ...         ...    ...\n","10975  quraishi      Arabic      0\n","10976   innalls     English      4\n","10977      król      Polish     12\n","10978    purvis     English      4\n","10979  messerli      German      6\n","\n","[10980 rows x 3 columns]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(\"surnames.csv\")\n","df[\"label\"] = le.fit_transform(df.nationality)\n","df.surname = df.surname.apply(lambda x:x.lower())\n","df"]},{"cell_type":"markdown","metadata":{"id":"FaqKKcKshWAp"},"source":["Разделение на обучающую и тестовую выборку"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"thpb2AWUhWAp","outputId":"708d31f6-adbd-41ee-ae57-d95b096e7df0"},"outputs":[{"data":{"text/plain":["[9882, 1098, 9882, 1098]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["X_train, X_test, y_train, y_test = model_selection.train_test_split( df.surname, df.label, test_size=0.1, random_state=42, statify=df.label)\n","\n","list(map(len,[X_train, X_test, y_train, y_test]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UGSoZWeahWAp"},"outputs":[],"source":["from collections import Counter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kUkSZkDqxNYS","outputId":"428924f1-055b-4de6-a13d-3c889493da3d"},"outputs":[{"name":"stdout","output_type":"stream","text":["forward {'è': 0, 'ç': 1, 'õ': 2, 'l': 3, 'ü': 4, 'n': 5, 'c': 6, 'ż': 7, 'v': 8, 'à': 9, 'k': 10, 'ń': 11, 'j': 12, 'f': 13, 'ß': 14, 'ś': 15, 'o': 16, 'ä': 17, 'ê': 18, 'r': 19, \"'\": 20, 'ñ': 21, 'í': 22, 'i': 23, 'w': 24, 'u': 25, 'x': 26, 'ù': 27, '/': 28, 'ó': 29, 'á': 30, 'h': 31, 'ö': 32, 'ì': 33, 'b': 34, 'd': 35, 'g': 36, 'ą': 37, ':': 38, 'ł': 39, 'e': 40, 'z': 41, '1': 42, 'ò': 43, 't': 44, 'ã': 45, 'p': 46, 'y': 47, 'a': 48, 's': 49, 'ú': 50, 'é': 51, 'q': 52, '-': 53, 'm': 54}\n","backward {0: 'è', 1: 'ç', 2: 'õ', 3: 'l', 4: 'ü', 5: 'n', 6: 'c', 7: 'ż', 8: 'v', 9: 'à', 10: 'k', 11: 'ń', 12: 'j', 13: 'f', 14: 'ß', 15: 'ś', 16: 'o', 17: 'ä', 18: 'ê', 19: 'r', 20: \"'\", 21: 'ñ', 22: 'í', 23: 'i', 24: 'w', 25: 'u', 26: 'x', 27: 'ù', 28: '/', 29: 'ó', 30: 'á', 31: 'h', 32: 'ö', 33: 'ì', 34: 'b', 35: 'd', 36: 'g', 37: 'ą', 38: ':', 39: 'ł', 40: 'e', 41: 'z', 42: '1', 43: 'ò', 44: 't', 45: 'ã', 46: 'p', 47: 'y', 48: 'a', 49: 's', 50: 'ú', 51: 'é', 52: 'q', 53: '-', 54: 'm'}\n"]}],"source":["class Vocab:  \n","  def __init__(self, data):\n","    letters = list(set(data))\n","    forward = dict((j,i) for i,j in enumerate(letters))\n","    backward = dict((j,i) for i,j in forward.items())\n","    print(\"forward\",forward)\n","    print(\"backward\",backward)\n","    self.idx_to_token = backward\n","    self.token_to_idx = forward\n","    self.vocab_len = len(forward)\n","\n","\n","vat = Vocab(\"\".join(df.surname.values))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9NGpAxK1hWAq"},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":229},"executionInfo":{"elapsed":1303,"status":"error","timestamp":1619117849212,"user":{"displayName":"Никита Блохин","photoUrl":"","userId":"16402972581398673009"},"user_tz":-180},"id":"WCaRK1QHxe0A","outputId":"5d1243af-d0dd-4922-9468-9618f5df4605","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["forward {'è': 0, 'ç': 1, 'õ': 2, 'l': 3, 'ü': 4, 'n': 5, 'c': 6, 'ż': 7, 'v': 8, 'à': 9, 'k': 10, 'ń': 11, 'j': 12, 'f': 13, 'ß': 14, 'ś': 15, 'o': 16, 'ä': 17, 'ê': 18, 'r': 19, \"'\": 20, 'ñ': 21, 'í': 22, 'i': 23, 'w': 24, 'u': 25, 'x': 26, 'ù': 27, '/': 28, 'ó': 29, 'á': 30, 'h': 31, 'ö': 32, 'ì': 33, 'b': 34, 'd': 35, 'g': 36, 'ą': 37, ':': 38, 'ł': 39, 'e': 40, 'z': 41, '1': 42, 'ò': 43, 't': 44, 'ã': 45, 'p': 46, 'y': 47, 'a': 48, 's': 49, 'ú': 50, 'é': 51, 'q': 52, '-': 53, 'm': 54}\n","backward {0: 'è', 1: 'ç', 2: 'õ', 3: 'l', 4: 'ü', 5: 'n', 6: 'c', 7: 'ż', 8: 'v', 9: 'à', 10: 'k', 11: 'ń', 12: 'j', 13: 'f', 14: 'ß', 15: 'ś', 16: 'o', 17: 'ä', 18: 'ê', 19: 'r', 20: \"'\", 21: 'ñ', 22: 'í', 23: 'i', 24: 'w', 25: 'u', 26: 'x', 27: 'ù', 28: '/', 29: 'ó', 30: 'á', 31: 'h', 32: 'ö', 33: 'ì', 34: 'b', 35: 'd', 36: 'g', 37: 'ą', 38: ':', 39: 'ł', 40: 'e', 41: 'z', 42: '1', 43: 'ò', 44: 't', 45: 'ã', 46: 'p', 47: 'y', 48: 'a', 49: 's', 50: 'ú', 51: 'é', 52: 'q', 53: '-', 54: 'm'}\n"]}],"source":["class SurnamesDataset(torch.utils.data.Dataset):\n","  def __init__(self, X, y, vocab: Vocab):\n","    self.X = X\n","    self.y = y\n","    self.vocab = vocab\n","\n","  def vectorize(self, surname):\n","    x = [0]* self.vocab.vocab_len\n","    for letter in surname:\n","        x[self.vocab.token_to_idx[letter]] = 1\n","    return x\n","    \n","  def __len__(self):\n","    return len(self.X)\n","\n","  def __getitem__(self, idx):\n","\n","    return torch.tensor(self.vectorize(self.X[idx])), self.y[idx]\n","\n","vat = Vocab(\"\".join(df.surname.values))\n","sur_train = SurnamesDataset(X_train.values,y_train.values,vat)\n","sur_test = SurnamesDataset(X_test.values,y_test.values,vat)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b_Z0KYUWhWAq"},"outputs":[],"source":["train_dataloader = torch.utils.data.DataLoader(sur_train, batch_size=64, shuffle=True)\n","test_dataloader = torch.utils.data.DataLoader(sur_test, batch_size=64, shuffle=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kjpv05hShWAr","outputId":"369e0046-d0f3-4164-8706-4ed1c5ac82f3"},"outputs":[{"data":{"text/plain":["(torch.Size([64, 55]),\n"," tensor([ 7,  8,  7,  3,  5,  7,  9,  5,  8,  5,  7,  7,  5,  5,  4,  2, 10,  9,\n","          4,  5,  5,  6,  6,  6,  5,  6,  7,  5,  4,  5, 10,  6,  9,  4,  9,  6,\n","          3,  6,  6,  5,  7,  4,  5, 10,  3,  6,  5,  6,  3,  6,  6,  5,  5,  8,\n","          6,  4,  6,  6,  3,  3,  4,  7,  5,  9]),\n"," tensor([15, 10,  6,  4,  4,  4, 14, 14,  7,  3,  4,  9, 14,  8,  2,  0, 14,  7,\n","         16,  4,  4, 14, 10,  0,  4, 14,  2,  1,  2,  0, 14,  4, 14,  0,  4,  2,\n","         14,  4, 14,  4, 14,  3, 17,  5,  0,  9,  2,  2,  0,  4,  9,  4, 14, 14,\n","          7,  6, 14,  4, 16,  1,  0, 16,  2, 14], dtype=torch.int32))"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["x,y = next(iter(train_dataloader)) \n","x.shape,torch.sum(x,axis=1),y # проверка размеров "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oFnFTrSOhWAr"},"outputs":[],"source":["import numpy as np\n","import torch\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","import seaborn as sns\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","from torchvision import models\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mv5dwuvYhWAs"},"outputs":[],"source":["class Classifier(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(55, 40)\n","        self.fc2 = nn.Linear(40, 20)\n","        self.fc3 = nn.Linear(20, 18)\n","        self.fc4 = nn.Linear(18, 18)\n","        \n","    def forward(self, x):        \n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        x = F.log_softmax(self.fc4(x), dim=1)\n","        \n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YVyvmvDnhWAs"},"outputs":[],"source":["model = Classifier()\n","criterion = nn.NLLLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"markdown","metadata":{"id":"xFdJBKt9hWAs"},"source":["переменная images использована из предыдущего пайплайна\n","Оставил ее такой же (#22)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJ-AAeyQhWAt","outputId":"d919c187-a6ae-4842-a763-8c5d2fae2c92"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50.. Training loss: 20.125.. Test loss: 2.091.. Test Accuracy: 0.405%\n","Epoch 2/50.. Training loss: 16.486.. Test loss: 1.879.. Test Accuracy: 0.459%\n","Epoch 3/50.. Training loss: 15.303.. Test loss: 1.793.. Test Accuracy: 0.470%\n","Epoch 4/50.. Training loss: 14.931.. Test loss: 1.760.. Test Accuracy: 0.489%\n","Epoch 5/50.. Training loss: 14.662.. Test loss: 1.705.. Test Accuracy: 0.505%\n","Epoch 6/50.. Training loss: 14.365.. Test loss: 1.678.. Test Accuracy: 0.509%\n","Epoch 7/50.. Training loss: 14.028.. Test loss: 1.650.. Test Accuracy: 0.507%\n","Epoch 8/50.. Training loss: 13.656.. Test loss: 1.607.. Test Accuracy: 0.529%\n","Epoch 9/50.. Training loss: 13.362.. Test loss: 1.599.. Test Accuracy: 0.523%\n","Epoch 10/50.. Training loss: 13.107.. Test loss: 1.589.. Test Accuracy: 0.530%\n","Epoch 11/50.. Training loss: 12.901.. Test loss: 1.545.. Test Accuracy: 0.536%\n","Epoch 12/50.. Training loss: 12.725.. Test loss: 1.535.. Test Accuracy: 0.543%\n","Epoch 13/50.. Training loss: 12.571.. Test loss: 1.526.. Test Accuracy: 0.553%\n","Epoch 14/50.. Training loss: 12.382.. Test loss: 1.483.. Test Accuracy: 0.539%\n","Epoch 15/50.. Training loss: 12.214.. Test loss: 1.468.. Test Accuracy: 0.564%\n","Epoch 16/50.. Training loss: 12.097.. Test loss: 1.471.. Test Accuracy: 0.556%\n","Epoch 17/50.. Training loss: 11.962.. Test loss: 1.422.. Test Accuracy: 0.581%\n","Epoch 18/50.. Training loss: 11.800.. Test loss: 1.417.. Test Accuracy: 0.574%\n","Epoch 19/50.. Training loss: 11.700.. Test loss: 1.458.. Test Accuracy: 0.560%\n","Epoch 20/50.. Training loss: 11.564.. Test loss: 1.440.. Test Accuracy: 0.579%\n","Epoch 21/50.. Training loss: 11.475.. Test loss: 1.389.. Test Accuracy: 0.573%\n","Epoch 22/50.. Training loss: 11.327.. Test loss: 1.423.. Test Accuracy: 0.576%\n","Epoch 23/50.. Training loss: 11.247.. Test loss: 1.407.. Test Accuracy: 0.566%\n","Epoch 24/50.. Training loss: 11.148.. Test loss: 1.375.. Test Accuracy: 0.582%\n","Epoch 25/50.. Training loss: 11.080.. Test loss: 1.355.. Test Accuracy: 0.599%\n","Epoch 26/50.. Training loss: 10.990.. Test loss: 1.359.. Test Accuracy: 0.590%\n","Epoch 27/50.. Training loss: 10.882.. Test loss: 1.345.. Test Accuracy: 0.600%\n","Epoch 28/50.. Training loss: 10.852.. Test loss: 1.353.. Test Accuracy: 0.599%\n","Epoch 29/50.. Training loss: 10.741.. Test loss: 1.336.. Test Accuracy: 0.602%\n","Epoch 30/50.. Training loss: 10.701.. Test loss: 1.334.. Test Accuracy: 0.601%\n","Epoch 31/50.. Training loss: 10.634.. Test loss: 1.358.. Test Accuracy: 0.608%\n","Epoch 32/50.. Training loss: 10.547.. Test loss: 1.351.. Test Accuracy: 0.597%\n","Epoch 33/50.. Training loss: 10.514.. Test loss: 1.309.. Test Accuracy: 0.613%\n","Epoch 34/50.. Training loss: 10.467.. Test loss: 1.312.. Test Accuracy: 0.614%\n","Epoch 35/50.. Training loss: 10.431.. Test loss: 1.300.. Test Accuracy: 0.611%\n","Epoch 36/50.. Training loss: 10.388.. Test loss: 1.347.. Test Accuracy: 0.608%\n","Epoch 37/50.. Training loss: 10.321.. Test loss: 1.304.. Test Accuracy: 0.603%\n","Epoch 38/50.. Training loss: 10.288.. Test loss: 1.314.. Test Accuracy: 0.606%\n","Epoch 39/50.. Training loss: 10.252.. Test loss: 1.314.. Test Accuracy: 0.607%\n","Epoch 40/50.. Training loss: 10.211.. Test loss: 1.354.. Test Accuracy: 0.599%\n","Epoch 41/50.. Training loss: 10.188.. Test loss: 1.285.. Test Accuracy: 0.620%\n","Epoch 42/50.. Training loss: 10.158.. Test loss: 1.312.. Test Accuracy: 0.609%\n","Epoch 43/50.. Training loss: 10.114.. Test loss: 1.330.. Test Accuracy: 0.596%\n","Epoch 44/50.. Training loss: 10.082.. Test loss: 1.292.. Test Accuracy: 0.604%\n","Epoch 45/50.. Training loss: 10.030.. Test loss: 1.287.. Test Accuracy: 0.615%\n","Epoch 46/50.. Training loss: 10.009.. Test loss: 1.276.. Test Accuracy: 0.614%\n","Epoch 47/50.. Training loss: 9.976.. Test loss: 1.291.. Test Accuracy: 0.623%\n","Epoch 48/50.. Training loss: 9.986.. Test loss: 1.325.. Test Accuracy: 0.608%\n","Epoch 49/50.. Training loss: 9.930.. Test loss: 1.292.. Test Accuracy: 0.621%\n","Epoch 50/50.. Training loss: 9.907.. Test loss: 1.271.. Test Accuracy: 0.621%\n"]}],"source":["epochs = 50\n","train_losses, test_losses = [], []\n","\n","for e in range(epochs):\n","    running_loss = 0\n","    for images, labels in train_dataloader:\n","        # images = images.view(images.shape[0], -1)\n","        images = images.type(torch.float)\n","        # print(images.shape)\n","        labels = labels.type(torch.LongTensor)\n","        optimizer.zero_grad()\n","        logps = model(images)\n","        # print(logps)\n","        # print(labels)\n","        loss = criterion(logps, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","    else:\n","        test_loss = 0\n","        accuracy = 0\n","        # Turn off gradients for validation, saves memory and computations\n","        with torch.no_grad():\n","            for test_images, test_labels in test_dataloader:\n","                test_images = test_images.type(torch.float)\n","                logps = model(test_images)\n","                test_labels = test_labels.type(torch.LongTensor)\n","                test_loss += criterion(logps, test_labels)\n","                ps = torch.exp(logps)\n","                top_p, top_class = ps.topk(1, dim=1)\n","                equals = top_class == test_labels.view(*top_class.shape)\n","                accuracy += torch.mean(equals.type(torch.FloatTensor))\n","        \n","        train_losses.append(running_loss/len(test_dataloader))\n","        test_losses.append(test_loss/len(test_dataloader))\n","        \n","        print(\"Epoch {}/{}..\".format(e+1, epochs),\n","              \"Training loss: {:.3f}..\".format(train_losses[-1]),\n","              \"Test loss: {:.3f}..\".format(test_losses[-1]),\n","              \"Test Accuracy: {:.3f}%\".format(accuracy/len(test_dataloader)))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kEFbdK7vhWAt","outputId":"56e3dd96-c44e-4df8-c82f-736d2c4f0073"},"outputs":[{"name":"stdout","output_type":"stream","text":["1.13.0+cpu\n"]}],"source":["print(torch.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ad-mal4MhWAu","outputId":"2ba4ceef-389c-4d96-c90f-537ff65ddae2"},"outputs":[{"data":{"text/plain":["(tensor([[0.9609, 0.0136, 0.0130]], grad_fn=<TopkBackward0>),\n"," tensor([[14,  2,  7]]),\n"," array(['Russian', 'Czech', 'Greek'], dtype=object))"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["def vectorize(surname):\n","    x = [0]* vat.vocab_len\n","    for letter in surname:\n","        x[vat.token_to_idx[letter]] = 1\n","    return x\n","data = torch.tensor(vectorize(\"kuznetsov\")).type(torch.float).unsqueeze(0)\n","# data.shape\n","# model(data)\n","top_p, top_class = torch.exp(model(data)).topk(3, dim=1)\n","\n","top_p, top_class, le.inverse_transform(top_class[0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBfZkyQZhWAu","outputId":"c3914d79-ece7-453e-99ab-ebb9b8bba2f4"},"outputs":[{"data":{"text/plain":["(tensor([[0.4514, 0.3569, 0.0437]], grad_fn=<TopkBackward0>),\n"," tensor([[14,  4,  2]]),\n"," array(['Russian', 'English', 'Czech'], dtype=object))"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["def vectorize(surname):\n","    x = [0]* vat.vocab_len\n","    for letter in surname:\n","        x[vat.token_to_idx[letter]] = 1\n","    return x\n","data = torch.tensor(vectorize(\"smirnov\")).type(torch.float).unsqueeze(0)\n","# data.shape\n","# model(data)\n","top_p, top_class = torch.exp(model(data)).topk(3, dim=1)\n","\n","top_p, top_class, le.inverse_transform(top_class[0])"]},{"cell_type":"markdown","metadata":{"id":"PLmDB3fJtVox"},"source":["## 3. Классификация обзоров ресторанов\n","\n","Датасет: https://disk.yandex.ru/d/nY1o70JtAuYa8g\n","\n","\n","3.1 Считать файл `yelp/raw_train.csv`. Оставить от исходного датасета 10% строчек.\n","\n","3.2 Воспользоваться функцией `preprocess_text` из 1.1 для обработки текста отзыва. Закодировать рейтинг числами, начиная с 0.\n","\n","3.3 Разбить датасет на обучающую и тестовую выборку\n","\n","3.4 Реализовать класс `Vocab` (токен = слово)\n","\n","3.5 Реализовать класс `ReviewDataset`\n","\n","3.6 Обучить классификатор\n","\n","3.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yNg5FZA0hWAw","outputId":"6654b3ab-9e75-408f-e1e9-d27706481b86"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Ordered a large Mango-Pineapple smoothie. Stay...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Quite a surprise!  \\n\\nMy wife and I loved thi...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>First I will say, this is a nice atmosphere an...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>I was overall pretty impressed by this hotel. ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>Video link at bottom review. Worst service I h...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label                                               text\n","0      0  Ordered a large Mango-Pineapple smoothie. Stay...\n","1      1  Quite a surprise!  \\n\\nMy wife and I loved thi...\n","2      0  First I will say, this is a nice atmosphere an...\n","3      1  I was overall pretty impressed by this hotel. ...\n","4      0  Video link at bottom review. Worst service I h..."]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(\"raw_test.csv\",header=None)\n","df.columns = [\"label\", \"text\"]\n","df.label = df.label-1\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zjhL8n0RhWAw","outputId":"fe0bd891-f9f4-4962-f7ae-7e675ce4f33f"},"outputs":[{"name":"stdout","output_type":"stream","text":["calculating tokens\n"]},{"data":{"text/plain":["['suede',\n"," 'mophead',\n"," 'conspicuously',\n"," 'randomly',\n"," 'foxx',\n"," 'checkins',\n"," 'chs',\n"," 'frickin',\n"," 'tequileria',\n"," 'needs',\n"," 'verified',\n"," 'overeating',\n"," 'unneeded',\n"," 'belting',\n"," 'productive',\n"," 'juicenot',\n"," 'jaclyn',\n"," 'bundlet',\n"," 'isconsidered',\n"," 'elizabeth',\n"," 'tortured',\n"," 'thr',\n"," 'penalties',\n"," 'rtm',\n"," 'griffin',\n"," 'nde',\n"," 'wiggling',\n"," 'optimism',\n"," 'voudrez',\n"," 'drifting',\n"," 'macy',\n"," 'secruity',\n"," 'complimentarily',\n"," 'betsey',\n"," 'plot',\n"," 'pla',\n"," 'occupiers',\n"," 'carmas',\n"," 'slidesi',\n"," 'coffee',\n"," 'huber',\n"," 'succomb',\n"," 'aria',\n"," 'ornaments',\n"," 'isi',\n"," 'choisir',\n"," 'smallish',\n"," 'earpierce',\n"," 'apprendre',\n"," 'mirroring',\n"," 'pins',\n"," 'attendee',\n"," 'allllllllllllllllllllllllll',\n"," 'outreach',\n"," 'crisp',\n"," 'schmmamit',\n"," 'estrellas',\n"," 'pyrex',\n"," 'pizzettes',\n"," 'recomended',\n"," 'nicolle',\n"," 'roach',\n"," 'dermaplaning',\n"," 'invitant',\n"," 'spoiled',\n"," 'stimmig',\n"," 'montesano',\n"," 'jurisdiction',\n"," 'habernero',\n"," 'toilet',\n"," 'portion',\n"," 'scarpetta',\n"," 'relative',\n"," 'eigentlich',\n"," 'panda',\n"," 'sunglass',\n"," 'revising',\n"," 'onlkxaqfq',\n"," 'chivalrously',\n"," 'choose',\n"," 'realise',\n"," 'recherch',\n"," 'release',\n"," 'boon',\n"," 'involuntarily',\n"," 'gruesome',\n"," 'sirens',\n"," 'nuisances',\n"," 'website',\n"," 'legendary',\n"," 'ned',\n"," 'caraffe',\n"," 'clavelbeet',\n"," 'wheres',\n"," 'gonzalez',\n"," 'connecticut',\n"," 'tracker',\n"," 'autour',\n"," 'dainty',\n"," 'technican',\n"," 'threatening',\n"," 'businessally',\n"," 'insults',\n"," 'electronics',\n"," 'forum',\n"," 'foist',\n"," 'elusive',\n"," 'pith',\n"," 'glowing',\n"," 'buttt',\n"," 'aesthetic',\n"," 'moot',\n"," 'dulce',\n"," 'mannequins',\n"," 'trails',\n"," 'greaat',\n"," 'uck',\n"," 'preteens',\n"," 'oriented',\n"," 'wriitng',\n"," 'decamped',\n"," 'nsa',\n"," 'stockyards',\n"," 'bajillion',\n"," 'rerang',\n"," 'hardship',\n"," 'churro',\n"," 'dynamics',\n"," 'yesitalian',\n"," 'mitgenommen',\n"," 'ems',\n"," 'pdfforms',\n"," 'quintessential',\n"," 'homage',\n"," 'challenged',\n"," 'transfattening',\n"," 'turon',\n"," 'barstool',\n"," 'whack',\n"," 'pg',\n"," 'pistache',\n"," 'leed',\n"," 'gelled',\n"," 'surounding',\n"," 'maidaffordableinaz',\n"," 'smorez',\n"," 'cousinssubs',\n"," 'aggrivated',\n"," 'mongolian',\n"," 'zeromercury',\n"," 'happenin',\n"," 'softened',\n"," 'soliel',\n"," 'hingegen',\n"," 'stampeded',\n"," 'anyone',\n"," 'beleeeve',\n"," 'musk',\n"," 'geographically',\n"," 'coronado',\n"," 'refer',\n"," 'saskia',\n"," 'mangled',\n"," 'longwinded',\n"," 'northwest',\n"," 'nefarious',\n"," 'leches',\n"," 'grew',\n"," 'crumble',\n"," 'approximation',\n"," 'brainchild',\n"," 'precisely',\n"," 'afternoonand',\n"," 'farewell',\n"," 'abonn',\n"," 'serum',\n"," 'manga',\n"," 'smg',\n"," 'fcrzte',\n"," 'differed',\n"," 'dnc',\n"," 'khourys',\n"," 'xrjw',\n"," 'baaaannnnggggiiiinnng',\n"," 'gfy',\n"," 'canx',\n"," 'accoustics',\n"," 'subtler',\n"," 'thong',\n"," 'officious',\n"," 'unbeaten',\n"," 'gefallen',\n"," 'favourable',\n"," 'favorite',\n"," 'clothe',\n"," 'savoring',\n"," 'skippable',\n"," 'roadblocks',\n"," 'ballerina',\n"," 'suggesstion',\n"," 'snobbish',\n"," 'dictates',\n"," 'darkly',\n"," 'petes',\n"," 'profiterole',\n"," 'tattoo',\n"," 'atherton',\n"," 'barbacue',\n"," 'larp',\n"," 'smellthe',\n"," 'rayner',\n"," 'relies',\n"," 'luxurious',\n"," 'dodge',\n"," 'tirer',\n"," 'chaffing',\n"," 'markskip',\n"," 'loitering',\n"," 'lisa',\n"," 'antacids',\n"," 'colombia',\n"," 'assistants',\n"," 'honk',\n"," 'taqueria',\n"," 'coladamust',\n"," 'soiree',\n"," 'reckoned',\n"," 'jugendstilgeb',\n"," 'screaming',\n"," 'laughably',\n"," 'fanfare',\n"," 'potter',\n"," 'kinna',\n"," 'phyto',\n"," 'tops',\n"," 'badged',\n"," 'controls',\n"," 'deidre',\n"," 'fantasti',\n"," 'terrain',\n"," 'chow',\n"," 'businessmy',\n"," 'withfabulous',\n"," 'uploaded',\n"," 'challah',\n"," 'libraries',\n"," 'harmonica',\n"," 'smashburgerrrr',\n"," 'titanium',\n"," 'everrr',\n"," 'thats',\n"," 'goodnesscaramelized',\n"," 'proselytizing',\n"," 'pronounce',\n"," 'sloppily',\n"," 'county',\n"," 'unsurprised',\n"," 'artristry',\n"," 'lousey',\n"," 'stinking',\n"," 'kino',\n"," 'sqft',\n"," 'damnnit',\n"," 'nur',\n"," 'refracted',\n"," 'constraint',\n"," 'au',\n"," 'communities',\n"," 'roundup',\n"," 'whizzed',\n"," 'jacks',\n"," 'tlahttp',\n"," 'drummers',\n"," 'yoganastix',\n"," 'punched',\n"," 'stube',\n"," 'farnon',\n"," 'salado',\n"," 'cuase',\n"," 'cris',\n"," 'conversational',\n"," 'winter',\n"," 'fulldefinately',\n"," 'saladfood',\n"," 'sticky',\n"," 'retraining',\n"," 'seamer',\n"," 'judy',\n"," 'dann',\n"," 'precheck',\n"," 'yelpcdn',\n"," 'coolata',\n"," 'slopped',\n"," 'lonnnnggg',\n"," 'horns',\n"," 'visitswe',\n"," 'matching',\n"," 'vegaspersonalshopper',\n"," 'hipaa',\n"," 'blowingly',\n"," 'returned',\n"," 'reversing',\n"," 'waltz',\n"," 'ambers',\n"," 'contacts',\n"," 'shephard',\n"," 'four',\n"," 'craveable',\n"," 'broker',\n"," 'getr',\n"," 'oranges',\n"," 'amanda',\n"," 'jobos',\n"," 'hyun',\n"," 'exceptionality',\n"," 'typ',\n"," 'garments',\n"," 'couldattitude',\n"," 'haw',\n"," 'overflavored',\n"," 'banquettes',\n"," 'definately',\n"," 'mangeuse',\n"," 'golfers',\n"," 'exclusions',\n"," 'grape',\n"," 'kicker',\n"," 'tobeys',\n"," 'tsukiji',\n"," 'rockefellerciopinnocrab',\n"," 'gospel',\n"," 'vases',\n"," 'tec',\n"," 'identified',\n"," 'handstand',\n"," 'gao',\n"," 'cinedome',\n"," 'track',\n"," 'mould',\n"," 'penises',\n"," 'sorge',\n"," 'reviewhalf',\n"," 'invigorated',\n"," 'rehh',\n"," 'shuckers',\n"," 'snubs',\n"," 'grist',\n"," 'dude',\n"," 'revolve',\n"," 'urself',\n"," 'hounding',\n"," 'png',\n"," 'momentary',\n"," 'beeline',\n"," 'scientific',\n"," 'littered',\n"," 'ecrasees',\n"," 'tentative',\n"," 'appearanceoysterbar',\n"," 'hap',\n"," 'daydrinking',\n"," 'hunnidd',\n"," 'railer',\n"," 'holler',\n"," 'endulgence',\n"," 'caprices',\n"," 'chandelier',\n"," 'blowjob',\n"," 'polaire',\n"," 'pens',\n"," 'pilons',\n"," 'cottages',\n"," 'transactions',\n"," 'tomaso',\n"," 'oomg',\n"," 'craig',\n"," 'ingestion',\n"," 'alcove',\n"," 'arpa',\n"," 'norris',\n"," 'donated',\n"," 'cookies',\n"," 'petal',\n"," 'upwardly',\n"," 'cheesecurdsyour',\n"," 'characterization',\n"," 'briefly',\n"," 'reputable',\n"," 'managerpayless',\n"," 'caus',\n"," 'romanization',\n"," 'precio',\n"," 'stac',\n"," 'adrift',\n"," 'environ',\n"," 'potheads',\n"," 'consumed',\n"," 'romano',\n"," 'leery',\n"," 'tomato',\n"," 'szolowics',\n"," 'irn',\n"," 'hookers',\n"," 'gramps',\n"," 'ungenie',\n"," 'then',\n"," 'kibbosh',\n"," 'near',\n"," 'cy',\n"," 'honouring',\n"," 'zevon',\n"," 'greyhound',\n"," 'accompanimentsamerica',\n"," 'reminiscentof',\n"," 'behandelt',\n"," 'selbe',\n"," 'falafel',\n"," 'spending',\n"," 'margareta',\n"," 'divides',\n"," 'fringe',\n"," 'pinky',\n"," 'atlantis',\n"," 'vichy',\n"," 'mclaren',\n"," 'cabbages',\n"," 'lesport',\n"," 'attentative',\n"," 'roughly',\n"," 'myegmzd',\n"," 'toydi',\n"," 'budge',\n"," 'boarded',\n"," 'influenced',\n"," 'cuz',\n"," 'hitman',\n"," 'eod',\n"," 'undemanding',\n"," 'carma',\n"," 'draining',\n"," 'betwixt',\n"," 'businessesand',\n"," 'deshalb',\n"," 'whale',\n"," 'freakish',\n"," 'selection',\n"," 'mediumkicken',\n"," 'rinaldi',\n"," 'wondered',\n"," 'peppercinis',\n"," 'schmuck',\n"," 'daredevil',\n"," 'arbitrarily',\n"," 'bhindi',\n"," 'arville',\n"," 'alwyas',\n"," 'reclines',\n"," 'einfach',\n"," 'pouches',\n"," 'autotrader',\n"," 'headlight',\n"," 'koobideh',\n"," 'handprints',\n"," 'wormlike',\n"," 'gargling',\n"," 'cheating',\n"," 'robiola',\n"," 'vinegarette',\n"," 'invested',\n"," 'etymology',\n"," 'mausoleum',\n"," 'outta',\n"," 'moses',\n"," 'rigmarole',\n"," 'cirviche',\n"," 'reward',\n"," 'spacey',\n"," 'yhis',\n"," 'shinko',\n"," 'upcharges',\n"," 'beerenstrauch',\n"," 'saline',\n"," 'milks',\n"," 'convey',\n"," 'cents',\n"," 'microwave',\n"," 'empenada',\n"," 'hoosiers',\n"," 'definetely',\n"," 'devyn',\n"," 'superfood',\n"," 'undershorts',\n"," 'melba',\n"," 'except',\n"," 'umkc',\n"," 'diesel',\n"," 'blackend',\n"," 'cure',\n"," 'busco',\n"," 'output',\n"," 'fanatical',\n"," 'fcltige',\n"," 'fabulously',\n"," 'lorsque',\n"," 'enacted',\n"," 'spchek',\n"," 'carbonera',\n"," 'subpur',\n"," 'amore',\n"," 'sneer',\n"," 'dogged',\n"," 'acm',\n"," 'zwingen',\n"," 'wontonsmongolian',\n"," 'dhl',\n"," 'bookings',\n"," 'provalone',\n"," 'luxory',\n"," 'alternate',\n"," 'sortiment',\n"," 'sailboats',\n"," 'vegetables',\n"," 'idaho',\n"," 'certainty',\n"," 'grillmaster',\n"," 'clucked',\n"," 'hodori',\n"," 'distaste',\n"," 'fearless',\n"," 'buuuut',\n"," 'them',\n"," 'ghastly',\n"," 'chests',\n"," 'gerardo',\n"," 'yuummmm',\n"," 'shapely',\n"," 'potties',\n"," 'organism',\n"," 'lady',\n"," 'retainer',\n"," 'quant',\n"," 'tricia',\n"," 'parenthesis',\n"," 'overexaggerating',\n"," 'hazels',\n"," 'understaffed',\n"," 'stuffy',\n"," 'ses',\n"," 'defunct',\n"," 'drizzles',\n"," 'produkten',\n"," 'nashty',\n"," 'clorene',\n"," 'algo',\n"," 'dfe',\n"," 'crusting',\n"," 'foodporn',\n"," 'deliciousthick',\n"," 'artist',\n"," 'rapides',\n"," 'zaui',\n"," 'misled',\n"," 'casita',\n"," 'dullest',\n"," 'undeserved',\n"," 'shouldn',\n"," 'glace',\n"," 'sumomaki',\n"," 'contestant',\n"," 'grading',\n"," 'overpay',\n"," 'allotments',\n"," 'legged',\n"," 'mediterranean',\n"," 'nique',\n"," 'prying',\n"," 'comfortableand',\n"," 'misplaced',\n"," 'karaage',\n"," 'circle',\n"," 'saladeverything',\n"," 'soon',\n"," 'papers',\n"," 'aspekte',\n"," 'piven',\n"," 'chocolateboth',\n"," 'foresee',\n"," 'ganouj',\n"," 'burrito',\n"," 'elise',\n"," 'lever',\n"," 'sukithe',\n"," 'faraway',\n"," 'starschicken',\n"," 'island',\n"," 'chifa',\n"," 'passably',\n"," 'ewwwseriously',\n"," 'disease',\n"," 'philippine',\n"," 'compass',\n"," 'staffing',\n"," 'liiiitle',\n"," 'tapes',\n"," 'managing',\n"," 'oredered',\n"," 'incorreclty',\n"," 'sweater',\n"," 'underutilized',\n"," 'latch',\n"," 'parce',\n"," 'emptiness',\n"," 'ber',\n"," 'concretes',\n"," 'desparate',\n"," 'caley',\n"," 'drunkest',\n"," 'refoul',\n"," 'globbed',\n"," 'paisiblement',\n"," 'bearnaise',\n"," 'morons',\n"," 'drying',\n"," 'uncleaned',\n"," 'equalled',\n"," 'siti',\n"," 'annissimply',\n"," 'saladthere',\n"," 'salaire',\n"," 'saltynow',\n"," 'guinea',\n"," 'partake',\n"," 'doubling',\n"," 'rhabarber',\n"," 'pepers',\n"," 'refreeze',\n"," 'measured',\n"," 'marcos',\n"," 'japon',\n"," 'halla',\n"," 'recap',\n"," 'mies',\n"," 'moosecock',\n"," 'concordia',\n"," 'conceptual',\n"," 'yrold',\n"," 'grabbed',\n"," 'waitin',\n"," 'stands',\n"," 'poster',\n"," 'russo',\n"," 'scrooge',\n"," 'foregone',\n"," 'saws',\n"," 'pairing',\n"," 'slyly',\n"," 'encouraging',\n"," 'portishead',\n"," 'beefy',\n"," 'ktar',\n"," 'sweetener',\n"," 'defrost',\n"," 'ping',\n"," 'afghan',\n"," 'neopolitan',\n"," 'pallazzo',\n"," 'threesome',\n"," 'katina',\n"," 'mosquitos',\n"," 'spewed',\n"," 'negatives',\n"," 'herzog',\n"," 'habachi',\n"," 'dork',\n"," 'raspados',\n"," 'newton',\n"," 'referesh',\n"," 'fragrance',\n"," 'convent',\n"," 'cringeworthy',\n"," 'accents',\n"," 'diese',\n"," 'wpuld',\n"," 'created',\n"," 'leeway',\n"," 'pinhead',\n"," 'psychedelic',\n"," 'gay',\n"," 'pssst',\n"," 'innenraum',\n"," 'spectacles',\n"," 'peppep',\n"," 'housewithin',\n"," 'unexplainable',\n"," 'businessman',\n"," 'peopel',\n"," 'stimuli',\n"," 'itauthentic',\n"," 'mikkie',\n"," 'variances',\n"," 'vocalize',\n"," 'differently',\n"," 'ftr',\n"," 'brilliants',\n"," 'groceries',\n"," 'sympathize',\n"," 'choads',\n"," 'cgthe',\n"," 'conspire',\n"," 'albeity',\n"," 'males',\n"," 'literally',\n"," 'amuses',\n"," 'containment',\n"," 'knowlegable',\n"," 'snaps',\n"," 'taek',\n"," 'donw',\n"," 'library',\n"," 'santisima',\n"," 'pickup',\n"," 'zang',\n"," 'hangin',\n"," 'chickenkimchi',\n"," 'systems',\n"," 'dentist',\n"," 'concurrently',\n"," 'clois',\n"," 'loanhead',\n"," 'mononcs',\n"," 'affich',\n"," 'ss',\n"," 'stork',\n"," 'lovee',\n"," 'organic',\n"," 'rife',\n"," 'degenerates',\n"," 'bubbly',\n"," 'traits',\n"," 'mersa',\n"," 'drugstore',\n"," 'pizzala',\n"," 'baiz',\n"," 'freut',\n"," 'classism',\n"," 'store',\n"," 'pdf',\n"," 'out',\n"," 'weaksauce',\n"," 'raquet',\n"," 'prefabbed',\n"," 'molding',\n"," 'boden',\n"," 'liu',\n"," 'unethcial',\n"," 'springy',\n"," 'hundres',\n"," 'hab',\n"," 'fac',\n"," 'surge',\n"," 'jez',\n"," 'flatiron',\n"," 'nitroglycerine',\n"," 'noticeably',\n"," 'certificated',\n"," 'desserts',\n"," 'hazardous',\n"," 'invalid',\n"," 'bachelor',\n"," 'cola',\n"," 'medicore',\n"," 'inconsiderate',\n"," 'reaches',\n"," 'dx',\n"," 'algebra',\n"," 'bogohappy',\n"," 'notbuca',\n"," 'dispatched',\n"," 'situated',\n"," 'bois',\n"," 'magner',\n"," 'norms',\n"," 'glen',\n"," 'altercation',\n"," 'personnellement',\n"," 'splashing',\n"," 'aspirations',\n"," 'jalepenos',\n"," 'clover',\n"," 'mongo',\n"," 'gladiator',\n"," 'primant',\n"," 'mafia',\n"," 'crabatini',\n"," 'oooo',\n"," 'vegasthe',\n"," 'loudly',\n"," 'bloodiness',\n"," 'wouldupon',\n"," 'reigns',\n"," 'libra',\n"," 'anchovy',\n"," 'asia',\n"," 'tuscan',\n"," 'what',\n"," 'breadbasket',\n"," 'fuckers',\n"," 'inconspicuous',\n"," 'eagerness',\n"," 'freeing',\n"," 'lickity',\n"," 'lowest',\n"," 'comedian',\n"," 'moons',\n"," 'lens',\n"," 'ghettotastic',\n"," 'fever',\n"," 'qwi',\n"," 'emporter',\n"," 'farebad',\n"," 'loooooooves',\n"," 'falsch',\n"," 'santa',\n"," 'lemongrass',\n"," 'futuresexlovesounds',\n"," 'speared',\n"," 'incredible',\n"," 'restrooms',\n"," 'disastrously',\n"," 'wallet',\n"," 'cuzz',\n"," 'videotape',\n"," 'vite',\n"," 'elderly',\n"," 'livelihood',\n"," 'battistas',\n"," 'competencies',\n"," 'chapman',\n"," 'laurie',\n"," 'pandemic',\n"," 'rajiv',\n"," 'gryo',\n"," 'pssh',\n"," 'ezty',\n"," 'pricewise',\n"," 'totaled',\n"," 'curing',\n"," 'pennies',\n"," 'exists',\n"," 'farro',\n"," 'blvd',\n"," 'signe',\n"," 'fchst',\n"," 'dungeons',\n"," 'meilleurs',\n"," 'giselle',\n"," 'bought',\n"," 'referring',\n"," 'profile',\n"," 'dustin',\n"," 'madden',\n"," 'balm',\n"," 'reedy',\n"," 'oddity',\n"," 'orchid',\n"," 'housebroken',\n"," 'toofless',\n"," 'attempting',\n"," 'magasin',\n"," 'oversights',\n"," 'sail',\n"," 'bruster',\n"," 'incomplete',\n"," 'rollergirl',\n"," 'gags',\n"," 'hostess',\n"," 'partnered',\n"," 'deceiving',\n"," 'subscribers',\n"," 'kheer',\n"," 'fumble',\n"," 'raving',\n"," 'fator',\n"," 'ojdr',\n"," 'cybershot',\n"," 'throttling',\n"," 'cautioned',\n"," 'podium',\n"," 'jokes',\n"," 'courge',\n"," 'brooker',\n"," 'monets',\n"," 'sued',\n"," 'stoplight',\n"," 'bareback',\n"," 'transition',\n"," 'tempered',\n"," 'almondmilk',\n"," 'consequences',\n"," 'scratched',\n"," 'lange',\n"," 'monumental',\n"," 'most',\n"," 'zteljlzafq',\n"," 'bygone',\n"," 'urusawa',\n"," 'bat',\n"," 'brixx',\n"," 'ooey',\n"," 'dreikosens',\n"," 'portionshave',\n"," 'casserole',\n"," 'wnba',\n"," 'mannilow',\n"," 'peeps',\n"," 'restriction',\n"," 'bested',\n"," 'numberous',\n"," 'kesslers',\n"," 'wereasked',\n"," 'stompers',\n"," 'takeover',\n"," 'citations',\n"," 'distractions',\n"," 'hahah',\n"," 'shhhhhh',\n"," 'resolutions',\n"," 'mcguinn',\n"," 'katz',\n"," 'chilequile',\n"," 'pinkman',\n"," 'disregarded',\n"," 'ludicrous',\n"," 'pretensious',\n"," 'putters',\n"," 'oban',\n"," 'pablum',\n"," 'dak',\n"," 'woos',\n"," 'ruling',\n"," 'obsessively',\n"," 'flimsiest',\n"," 'ejuice',\n"," 'surveilled',\n"," 'blan',\n"," 'stuffier',\n"," 'cavoletti',\n"," 'lelecke',\n"," 'copines',\n"," 'sharkbowl',\n"," 'firenze',\n"," 'mutma',\n"," 'veils',\n"," 'sportscenter',\n"," 'petfinder',\n"," 'faxed',\n"," 'appr',\n"," 'eskew',\n"," 'gorgeious',\n"," 'ingrown',\n"," 'friend',\n"," 'waitress',\n"," 'peter',\n"," 'toys',\n"," 'tabatha',\n"," 'souffle',\n"," 'pipes',\n"," 'daughter',\n"," 'visitors',\n"," 'prenez',\n"," 'unhappier',\n"," 'ungenerous',\n"," 'cherylynn',\n"," 'figurine',\n"," 'includes',\n"," 'cupboards',\n"," 'versucht',\n"," 'arctic',\n"," 'billigen',\n"," 'yummi',\n"," 'rocket',\n"," 'harassment',\n"," 'assiette',\n"," 'soyo',\n"," 'expedia',\n"," 'simpo',\n"," 'tranny',\n"," 'wthe',\n"," 'warehouses',\n"," 'caeser',\n"," 'rumjungle',\n"," 'feathers',\n"," 'stopoff',\n"," 'han',\n"," 'amazing',\n"," 'video',\n"," 'aubrey',\n"," 'scotsman',\n"," 'phu',\n"," 'none',\n"," ...]"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["X = df.text.apply(lambda x: preprocess_text(x.replace(\"\\\\n\", \"\").replace(\".\",\" \"))).values\n","y = df.label.values\n","\n","print(\"calculating tokens\")\n","\n","tokens = list(\n","    set(\n","        word_tokenize(\n","                \" \".join(\n","                    X\n","                )\n","        )\n","    )\n",")\n","tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"doym137NhWAx"},"outputs":[],"source":["class Vocab: # что мне снова с этим делать?\n","  def __init__(self, all_tokens):\n","    # self.idx_to_token = ...\n","    # self.token_to_idx = ...\n","    # self.vocab_len = ...\n","    forward = dict((j,i) for i,j in enumerate(all_tokens))\n","    backward = dict((j,i) for i,j in forward.items())\n","    self.idx_to_token = backward\n","    self.token_to_idx = forward\n","    self.vocab_len = len(forward)\n","vat = Vocab(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QCET6Ly1hWAy","outputId":"6e4163d7-9828-4e7b-d0c1-b93dfcfb481e"},"outputs":[{"data":{"text/plain":["[30400, 7600, 30400, 7600]"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["X_train, X_test, y_train, y_test = model_selection.train_test_split( X, y, test_size=0.2, random_state=42)\n","\n","list(map(len,[X_train, X_test, y_train, y_test]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXLmCDvcvRmb"},"outputs":[],"source":["class ReviewDataset1(torch.utils.data.Dataset):\n","  def __init__(self, X, y, vocab: Vocab):\n","    self.X = X\n","    self.y = y\n","    self.vocab = vocab\n","\n","  def vectorize(self, review):\n","    \"\"\"Генерирует представление отзыва review при помощи бинарного кодирования (см. 1.2)\"\"\"\n","    x = [0] * self.vocab.vocab_len\n","\n","    for letter in word_tokenize(preprocess_text(review.replace(\"\\\\n\", \"\").replace(\".\", \" \"))):\n","      x[self.vocab.token_to_idx[letter]] = 1\n","    return x\n","\n","  def __len__(self):\n","    return len(self.X)\n","\n","  def __getitem__(self, idx):\n","    return torch.tensor(self.vectorize(self.X[idx])), self.y[idx]\n","\n","rev_train = ReviewDataset1(X_train,y_train,vat)\n","rev_test = ReviewDataset1(X_test,y_test,vat)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_jEGYzmlhWAy"},"outputs":[],"source":["import numpy as np\n","import torch\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","import seaborn as sns\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","from torchvision import models\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2wN1sojihWAz"},"outputs":[],"source":["train_dataloader = torch.utils.data.DataLoader(rev_train, batch_size=64, shuffle=True)\n","test_dataloader = torch.utils.data.DataLoader(rev_test, batch_size=64, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SfEylK87hWAz","outputId":"df6b3359-89c7-4761-80a3-d71d21721e8d"},"outputs":[{"data":{"text/plain":["(torch.Size([64, 56938]),\n"," tensor([ 34, 103,  19, 177,  77,  59,  26,  78, 339,  37,  55,  74, 162,  22,\n","          55,  63,  83,  83,  46,  54,  79, 162,  60, 103,  43,  23,  44,  41,\n","          76,  24,  51,  61,  41,  17,  83,  17,  54, 264,  85,  56,  48,  98,\n","          76,  98,  70,  48,  10, 257, 117,  32, 168, 132,  75,  59, 112,  86,\n","          56, 202,  66,  37,  56,  57,  56, 131]),\n"," tensor([1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,\n","         1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n","         0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0]))"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["x, y = next(iter(train_dataloader))\n","x.shape,torch.sum(x,axis=1),y # проверка размеров "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sOKojHBYhWAz"},"outputs":[],"source":["class Classifier(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(56938, 3_000)\n","        self.fc2 = nn.Linear(3_000, 2_000)\n","        self.fc3 = nn.Linear(2_000, 500)\n","        self.fc4 = nn.Linear(500, 2)\n","        \n","    def forward(self, x):        \n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        x = F.log_softmax(self.fc4(x), dim=1)\n","        \n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C3YA7X7dhWAz"},"outputs":[],"source":["model = Classifier()\n","criterion = nn.NLLLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["cd8ae63bf4364fa5a0a29c7cffa8a1a8","0c0d4170e46d42f99fa3842028af9057","17b288270a2a4e1ebdc69486bc9d4511"]},"id":"RZO2mcu2hWAz","outputId":"b549b10a-e785-4a0c-80bb-9bccafd3f484"},"outputs":[{"data":{"application/json":{"ascii":false,"bar_format":null,"colour":null,"elapsed":0.014333009719848633,"initial":0,"n":0,"ncols":null,"nrows":null,"postfix":null,"prefix":"","rate":null,"total":475,"unit":"it","unit_divisor":1000,"unit_scale":false},"application/vnd.jupyter.widget-view+json":{"model_id":"cd8ae63bf4364fa5a0a29c7cffa8a1a8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/475 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 1/10.. Training loss: 1.042.. Test loss: 0.224.. Test Accuracy: 0.916%\n"]},{"data":{"application/json":{"ascii":false,"bar_format":null,"colour":null,"elapsed":0.013679742813110352,"initial":0,"n":0,"ncols":null,"nrows":null,"postfix":null,"prefix":"","rate":null,"total":475,"unit":"it","unit_divisor":1000,"unit_scale":false},"application/vnd.jupyter.widget-view+json":{"model_id":"0c0d4170e46d42f99fa3842028af9057","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/475 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 2/10.. Training loss: 0.376.. Test loss: 0.368.. Test Accuracy: 0.905%\n"]},{"data":{"application/json":{"ascii":false,"bar_format":null,"colour":null,"elapsed":0.014306306838989258,"initial":0,"n":0,"ncols":null,"nrows":null,"postfix":null,"prefix":"","rate":null,"total":475,"unit":"it","unit_divisor":1000,"unit_scale":false},"application/vnd.jupyter.widget-view+json":{"model_id":"17b288270a2a4e1ebdc69486bc9d4511","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/475 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m/root/test_images/CIFAR-10/min/05_NLP_1_intro.ipynb Cell 45\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmsu/root/test_images/CIFAR-10/min/05_NLP_1_intro.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmsu/root/test_images/CIFAR-10/min/05_NLP_1_intro.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     running_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bmsu/root/test_images/CIFAR-10/min/05_NLP_1_intro.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m tqdm(train_dataloader):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmsu/root/test_images/CIFAR-10/min/05_NLP_1_intro.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m# print(\"1111\")\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bmsu/root/test_images/CIFAR-10/min/05_NLP_1_intro.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m# images = images.view(images.shape[0], -1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmsu/root/test_images/CIFAR-10/min/05_NLP_1_intro.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mtype(torch\u001b[39m.\u001b[39mfloat)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bmsu/root/test_images/CIFAR-10/min/05_NLP_1_intro.ipynb#Y110sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m# print(images.shape)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/notebook.py:258\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m--> 258\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    259\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    261\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:578\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    577\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    579\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    581\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    582\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py:618\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    619\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    620\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n","File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:175\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    172\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m [default_collate(samples) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/collate.py:141\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    139\u001b[0m         storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    140\u001b[0m         out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n\u001b[1;32m    142\u001b[0m \u001b[39melif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstr_\u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[1;32m    143\u001b[0m         \u001b[39mand\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstring_\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mndarray\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m elem_type\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmemmap\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    145\u001b[0m         \u001b[39m# array of string classes and object\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["epochs = 10\n","train_losses, test_losses = [], []\n","\n","model = model.to(\"cuda\")\n","for e in range(epochs):\n","    running_loss = 0\n","    for images, labels in tqdm(train_dataloader):\n","        # print(\"1111\")\n","        # images = images.view(images.shape[0], -1)\n","        images = images.type(torch.float).to(\"cuda\")\n","        # print(images.shape)\n","        optimizer.zero_grad()\n","        logps = model(images)\n","        # print(logps)\n","        # print(labels)\n","        loss = criterion(logps, labels.to(\"cuda\"))\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","    else:\n","        test_loss = 0\n","        accuracy = 0\n","        # Turn off gradients for validation, saves memory and computations\n","        with torch.no_grad():\n","            for test_images, test_labels in test_dataloader:\n","                test_images = test_images.type(torch.float).to(\"cuda\")\n","                logps = model(test_images)\n","                test_loss += criterion(logps, test_labels.to(\"cuda\"))\n","                ps = torch.exp(logps).detach().cpu()\n","                top_p, top_class = ps.topk(1, dim=1)\n","                equals = top_class == test_labels.view(*top_class.shape)\n","                accuracy += torch.mean(equals.type(torch.FloatTensor))\n","        \n","        train_losses.append(running_loss/len(test_dataloader))\n","        test_losses.append(test_loss/len(test_dataloader))\n","        \n","        print(\"Epoch {}/{}..\".format(e+1, epochs),\n","              \"Training loss: {:.3f}..\".format(train_losses[-1]),\n","              \"Test loss: {:.3f}..\".format(test_losses[-1]),\n","              \"Test Accuracy: {:.3f}%\".format(accuracy/len(test_dataloader)))"]},{"cell_type":"markdown","metadata":{"id":"ApkE2T4dhWA0"},"source":["1 - положительный отзыв, 0 - отрицательный"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"coe5By1EhWA0","outputId":"276727ef-bf97-4a80-8ea2-9511c989e3f6"},"outputs":[{"data":{"text/plain":["torch.return_types.topk(\n","values=tensor([[0.9789]], device='cuda:0', grad_fn=<TopkBackward0>),\n","indices=tensor([[1]], device='cuda:0'))"]},"execution_count":154,"metadata":{},"output_type":"execute_result"}],"source":["data = torch.tensor(rev_train.vectorize(\"good service i like it my wife become pregnant\")).type(torch.float).unsqueeze(0)\n","torch.exp(model(data.to(\"cuda\"))).topk(1, dim=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_w5C4WWhWA0","outputId":"14c2f763-abb9-4be7-cecf-1a275c760513"},"outputs":[{"data":{"text/plain":["torch.return_types.topk(\n","values=tensor([[0.9787]], device='cuda:0', grad_fn=<TopkBackward0>),\n","indices=tensor([[0]], device='cuda:0'))"]},"execution_count":155,"metadata":{},"output_type":"execute_result"}],"source":["data = torch.tensor(rev_train.vectorize(\"bad service i dislike it my husband become pregnant\")).type(torch.float).unsqueeze(0)\n","torch.exp(model(data.to(\"cuda\"))).topk(1, dim=1)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.8.1 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.1"},"vscode":{"interpreter":{"hash":"4bcec76e9f1100dae2af6e03f3527ce3587d1a3e9624c2a8f17d49adf70cee33"}}},"nbformat":4,"nbformat_minor":0}