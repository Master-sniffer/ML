{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkBPcTgxP8LjcV4DGHTjrL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7V9jTCm9HPz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Design model (input, output size, forward pass)\n",
        "#Construct loss and optimizer\n",
        "#training loop:\n",
        "  #forward pass: compute prediction\n",
        "  #backward pass: gradients\n",
        "  #update weights\n",
        "\n",
        "X = torch.tensor([[1],[2],[3],[4]] , dtype=torch.float32)\n",
        "Y = torch.tensor([[2],[4],[6],[8]] , dtype=torch.float32)\n",
        "\n",
        "X_test = torch.tensor([5] , dtype=torch.float32)\n",
        "\n",
        "n_samples , n_features = X.shape\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "#model = nn.Linear(input_size,output_size)\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self,input_dim,output_dim):\n",
        "    super(LinearRegression, self).__init__()\n",
        "    self.lin = nn.Linear(input_dim,output_dim)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size, output_size)"
      ],
      "metadata": {
        "id": "eyx-JV6H9LXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Before prediction f(5) = {model(X_test).item()} \")\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  y_pred = model(X)\n",
        "\n",
        "  #loss\n",
        "  l = loss(Y,y_pred)\n",
        "\n",
        "  # gradient = backward pass\n",
        "  l.backward()\n",
        "\n",
        "  #update weights\n",
        "  optimizer.step()\n",
        "\n",
        "  #zero gradient\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch%2 == 0):\n",
        "    [w,b] = model.parameters()\n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f} loss = {l:.5f}')\n",
        "  \n",
        "print(f\"prediction after training {model(X_test).item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zT-QTzix-G4T",
        "outputId": "1a968512-0fde-4637-c94e-03816ba93bbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before prediction f(5) = -3.83012056350708 \n",
            "epoch 1: w = -0.178 loss = 61.90804\n",
            "epoch 3: w = 0.479 loss = 29.80745\n",
            "epoch 5: w = 0.935 loss = 14.35204\n",
            "epoch 7: w = 1.252 loss = 6.91074\n",
            "epoch 9: w = 1.472 loss = 3.32799\n",
            "epoch 11: w = 1.624 loss = 1.60300\n",
            "epoch 13: w = 1.730 loss = 0.77247\n",
            "epoch 15: w = 1.804 loss = 0.37258\n",
            "epoch 17: w = 1.855 loss = 0.18005\n",
            "epoch 19: w = 1.890 loss = 0.08734\n",
            "epoch 21: w = 1.915 loss = 0.04269\n",
            "epoch 23: w = 1.932 loss = 0.02119\n",
            "epoch 25: w = 1.944 loss = 0.01083\n",
            "epoch 27: w = 1.953 loss = 0.00583\n",
            "epoch 29: w = 1.958 loss = 0.00342\n",
            "epoch 31: w = 1.963 loss = 0.00225\n",
            "epoch 33: w = 1.965 loss = 0.00168\n",
            "epoch 35: w = 1.968 loss = 0.00140\n",
            "epoch 37: w = 1.969 loss = 0.00126\n",
            "epoch 39: w = 1.970 loss = 0.00118\n",
            "epoch 41: w = 1.971 loss = 0.00114\n",
            "epoch 43: w = 1.972 loss = 0.00111\n",
            "epoch 45: w = 1.972 loss = 0.00109\n",
            "epoch 47: w = 1.972 loss = 0.00107\n",
            "epoch 49: w = 1.973 loss = 0.00106\n",
            "epoch 51: w = 1.973 loss = 0.00105\n",
            "epoch 53: w = 1.973 loss = 0.00103\n",
            "epoch 55: w = 1.973 loss = 0.00102\n",
            "epoch 57: w = 1.974 loss = 0.00101\n",
            "epoch 59: w = 1.974 loss = 0.00100\n",
            "epoch 61: w = 1.974 loss = 0.00099\n",
            "epoch 63: w = 1.974 loss = 0.00097\n",
            "epoch 65: w = 1.974 loss = 0.00096\n",
            "epoch 67: w = 1.974 loss = 0.00095\n",
            "epoch 69: w = 1.975 loss = 0.00094\n",
            "epoch 71: w = 1.975 loss = 0.00093\n",
            "epoch 73: w = 1.975 loss = 0.00092\n",
            "epoch 75: w = 1.975 loss = 0.00091\n",
            "epoch 77: w = 1.975 loss = 0.00090\n",
            "epoch 79: w = 1.975 loss = 0.00088\n",
            "epoch 81: w = 1.975 loss = 0.00087\n",
            "epoch 83: w = 1.976 loss = 0.00086\n",
            "epoch 85: w = 1.976 loss = 0.00085\n",
            "epoch 87: w = 1.976 loss = 0.00084\n",
            "epoch 89: w = 1.976 loss = 0.00083\n",
            "epoch 91: w = 1.976 loss = 0.00082\n",
            "epoch 93: w = 1.976 loss = 0.00081\n",
            "epoch 95: w = 1.976 loss = 0.00080\n",
            "epoch 97: w = 1.977 loss = 0.00079\n",
            "epoch 99: w = 1.977 loss = 0.00078\n",
            "prediction after training 9.952265739440918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZbWxGUh4B7kL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}