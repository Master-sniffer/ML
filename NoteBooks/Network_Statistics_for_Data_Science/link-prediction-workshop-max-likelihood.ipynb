{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Link prediction - workshop maximum likelihood based methods","metadata":{}},{"cell_type":"markdown","source":"In this workshop we look at two methods for link prediction: **similarity based methods** and **maximum likelihood (ML) based methods**. The main idea behind similarity based methods, is that you choose a similarity metric (like the number of common neighbours) and base the reliability of an edge on the score from this metric. The art of this method is choosing a suitable similarity metric for your problem.\n\nThe idea behind ML methods is taking an underlying network model from which we assume the observed network was generated. Then, given the chosen model, we can compute the conditional probability that a certain edge is present in the graph given the observed network (using Bayes' rule). The higher this probability, the more likely it is that the link is missing. The art behind this method is choosing a suitable underlying random graph model your network is a realisation from. Sadly, ML methods are often difficult to implement. Therefore, we will look at one specific ML method in this workshop: one based on the **stochastic block model (SBM)**. ","metadata":{}},{"cell_type":"markdown","source":"**Exercise 1.** Give two reasons why the stochastic block model might be a good underlying model for real-life networks. Also, give two reasons why is might be poorly suited for real-life networks. ","metadata":{}},{"cell_type":"markdown","source":"## The likelihood function of the stochastic block model","metadata":{}},{"cell_type":"markdown","source":"The centrepiece of any ML method is the **likelihood function**. This function takes a network as input, and outputs the probability of this network appearing. For the stochastic block model, we know that each edge is present independently of the others with a probability that depends on the types of its adjacent nodes. If we know the stochastic block model's inputs $\\vec{n}$ and $\\textbf{P}$, then we can deduce that the likelihood function of a graph $G$ in this model is given by $$\\mathbb{P}(G \\;|\\; \\vec{n}, \\textbf{P}) = \\prod_{t \\leq s} p_{ts}^{\\ell_{ts}} (1 - p_{ts})^{r_{ts} - \\ell_{ts}}.$$ Here, $\\ell_{ts}$ is the number of links between vertex-types $t$ and $s$, and $r_{ts}$ is the maximum number of possible links between types $t$ and $s$. Finally, $p_{ts}$ is the entry at row $t$ and column $s$ of the matrix $\\textbf{P}$, i.e. the probability that an edge between vertex-types $t$ and $s$ is present.","metadata":{}},{"cell_type":"markdown","source":"**Exercise 2.** Explain that the above likelihood function is correct. Then, implement it below. Test it on the generated stochastic block model underneath the function. *Hint: If implemented correctly, the likelihood should be approximately $6.25 \\cdot 10^{-15}$.*","metadata":{}},{"cell_type":"code","source":"import networkx as nx\nimport numpy as np\n\ndef cLSBM(G, n, P):\n    # Initialize likelihood\n    likelihood = 1.0\n\n    # Get the number of blocks\n    k = len(n)\n    \n    # Get the node labels for each block\n    node_blocks = {}\n    start = 0\n    for block_index in range(k):\n        for node in range(start, start + n[block_index]):\n            node_blocks[node] = block_index\n        start += n[block_index]\n    \n    # Initialize block pair edge and possible edge counts\n    ell = np.zeros((k, k))  # Number of edges between block pairs\n    r = np.zeros((k, k))    # Maximum number of possible edges between block pairs\n    \n    # Calculate ell and r values\n    for i in range(len(G.nodes)):\n        for j in range(i + 1, len(G.nodes)):\n            block_i = node_blocks[i]\n            block_j = node_blocks[j]\n            \n            # Update the maximum number of possible edges r\n            r[block_i][block_j] += 1\n            if block_i != block_j:\n                r[block_j][block_i] += 1\n            \n            # Update the number of observed edges ell\n            if G.has_edge(i, j):\n                ell[block_i][block_j] += 1\n                if block_i != block_j:\n                    ell[block_j][block_i] += 1\n\n    # For intra-block possible edges (r values)\n    for block in range(k):\n        r[block][block] = (n[block] * (n[block] - 1)) / 2  # Combinations of 2 within the same block\n    \n    # Compute the likelihood\n    for t in range(k):\n        for s in range(t, k):\n            p_ts = P[t][s]\n            ell_ts = ell[t][s]\n            r_ts = r[t][s]\n            \n            # Calculate likelihood component\n            if r_ts > 0:  # To avoid log(0) issues\n                likelihood *= (p_ts ** ell_ts) * ((1 - p_ts) ** (r_ts - ell_ts))\n    \n    return likelihood\n\n# The code below tests your method\n\nseed = 1 # Setting a random seed\n\n# Generating the SBM\nn = [5, 5, 5]\nP = [[0.5, 0.1, 0.02], [0.1, 0.7, 0.1], [0.02, 0.1, 0.3]]\nG = nx.stochastic_block_model(n, P, seed=seed)\n\n# Computing likelihood\nresult = cLSBM(G, n, P)\nprint(result)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T08:37:58.542536Z","iopub.execute_input":"2024-09-16T08:37:58.542945Z","iopub.status.idle":"2024-09-16T08:37:58.563594Z","shell.execute_reply.started":"2024-09-16T08:37:58.542906Z","shell.execute_reply":"2024-09-16T08:37:58.561730Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"6.254395434954231e-15\n","output_type":"stream"}]},{"cell_type":"markdown","source":"From Exercise 2 you should notice that the likelihood for a relatively small network is already quite low. This is to be expected, since you are multiplying many numbers that are smaller than $1$. However, since the numbers are so small, this will quickly cause problems when networks become big: the likelihood function will always output the value $0$. Hence, we often need tricks to ensure the likelihood remains meaningful. The idea behind all such tricks is noting that we do not need to use the pure conditional probability $\\mathbb{P}(G \\;|\\; \\vec{n}, \\textbf{P})$ as our reliability. ","metadata":{}},{"cell_type":"markdown","source":"**Exercise 3$\\star$.** Find a way to adapt your answer to Exercise 2 that is also useful for larger graphs. Implement it in the code block below, and test it on a larger SBM.","metadata":{}},{"cell_type":"code","source":"import networkx as nx\nimport numpy as np\n\ndef cLogLSBM(G, n, P):\n    # Initialize log-likelihood\n    log_likelihood = 0.0\n\n    # Get the number of blocks\n    k = len(n)\n    \n    # Get the node labels for each block\n    node_blocks = {}\n    start = 0\n    for block_index in range(k):\n        for node in range(start, start + n[block_index]):\n            node_blocks[node] = block_index\n        start += n[block_index]\n    \n    # Initialize block pair edge and possible edge counts\n    ell = np.zeros((k, k))  # Number of edges between block pairs\n    r = np.zeros((k, k))    # Maximum number of possible edges between block pairs\n    \n    # Calculate ell and r values\n    for i in range(len(G.nodes)):\n        for j in range(i + 1, len(G.nodes)):\n            block_i = node_blocks[i]\n            block_j = node_blocks[j]\n            \n            # Update the maximum number of possible edges r\n            r[block_i][block_j] += 1\n            if block_i != block_j:\n                r[block_j][block_i] += 1\n            \n            # Update the number of observed edges ell\n            if G.has_edge(i, j):\n                ell[block_i][block_j] += 1\n                if block_i != block_j:\n                    ell[block_j][block_i] += 1\n\n    # For intra-block possible edges (r values)\n    for block in range(k):\n        r[block][block] = (n[block] * (n[block] - 1)) / 2  # Combinations of 2 within the same block\n    \n    # Compute the log-likelihood\n    for t in range(k):\n        for s in range(t, k):\n            p_ts = P[t][s]\n            ell_ts = ell[t][s]\n            r_ts = r[t][s]\n            \n            # Calculate log-likelihood component\n            if r_ts > 0:  # To avoid log(0) issues\n                if p_ts > 0:\n                    log_likelihood += ell_ts * np.log(p_ts)\n                if p_ts < 1:\n                    log_likelihood += (r_ts - ell_ts) * np.log(1 - p_ts)\n    \n    return log_likelihood\n\n# The code below tests your method\n\nseed = 42 # Setting a random seed\n\n# Generating the SBM\nn = [50, 50, 50]\nP = [[0.5, 0.1, 0.02], [0.1, 0.7, 0.1], [0.02, 0.1, 0.3]]\nG = nx.stochastic_block_model(n, P, seed=seed)\n\n# Computing log-likelihood\nresult = cLogLSBM(G, n, P)\nprint(result)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T08:37:58.566686Z","iopub.execute_input":"2024-09-16T08:37:58.567270Z","iopub.status.idle":"2024-09-16T08:37:58.633243Z","shell.execute_reply.started":"2024-09-16T08:37:58.567214Z","shell.execute_reply":"2024-09-16T08:37:58.632058Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"-4247.370048761878\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The likelihood function for the SBM we have now is relatively simple to use. Moreover, if $T_i$ is the type of vertex $i$ and $T_j$ the type of vertex $j$, then we also know that the likelihood of edge $\\{i, j\\}$ is given by entry $(T_i, T_j)$ in matrix $\\textbf{P}$ (i.e., the value $p_{T_i, T_j}$). However, this analysis suffers from a major problem: it assumes we know the underlying SBM *exactly*. When we observe a network, however, we do not know the underlying SBM. All matrices $\\textbf{P}$ and vectors $\\vec{n}$ are fair game. Hence, to find the true likelihood, we need to integrate over all possible $\\textbf{P}$ and $\\vec{n}$.\n\nLet us first see what happens if we do not know $\\textbf{P}$, but do know $\\vec{n}$ (and the exact allocation of types to vertices). In that case, if we observe a graph $G^O$ the reliability/likelihood will be given by $\\mathbb{P}(\\{i, j\\} \\in E \\;|\\; \\vec{n}, G^O).$ Since we know the value of $\\mathbb{P}(\\{i, j\\} \\in E \\;|\\; \\vec{n}, \\text{P}, G^O) = p_{T_i T_j}$ we want to use the *law of total probability* to condition on the values in $\\text{P}$. This yields the following integral: $$\\mathbb{P}(\\{i, j\\} \\in E \\;|\\; \\vec{n}, G^O) = \\iint_{[0, 1]^g} \\mathbb{P}(\\{i, j\\} \\in E \\;|\\; \\vec{n}, \\textbf{P}, G^O) \\cdot \\mathbb{P}(\\textbf{P} \\;|\\; \\vec{n}, G^O) \\; \\text{d}\\textbf{P}.$$\nHere, $g$ is the number of distinct vertex-type pairs that are possible. Substituting what we know yields $$\\mathbb{P}(\\{i, j\\} \\in E \\;|\\; \\vec{n}, G^O) = \\iint_{[0, 1]^g} p_{T_i T_j} \\cdot \\mathbb{P}(\\textbf{P} \\;|\\; \\vec{n}, G^O) \\; \\text{d}\\textbf{P}.$$\n\nNow, to compute $\\mathbb{P}(\\textbf{P} \\;|\\; \\vec{n}, G^O)$ we will use *Bayes' rule* to rewrite $$\\mathbb{P}(\\textbf{P} \\;|\\; \\vec{n}, G^O) = \\frac{\\mathbb{P}(G^O \\;|\\; \\vec{n}, \\textbf{P} ) \\mathbb{P}(\\textbf{P}' \\;|\\; \\vec{n})}{\\iint_{[0, 1]^g} \\mathbb{P}(G^O \\;|\\; \\vec{n}, \\textbf{P}' ) \\mathbb{P}(\\textbf{P}' \\;|\\; \\vec{n}) \\; \\text{d}\\textbf{P}'} = \\frac{\\prod_{t \\leq s} p_{ts}^{\\ell_{ts}} (1 - p_{ts})^{r_{ts} - \\ell_{ts}} \\mathbb{P}(\\textbf{P} \\;|\\; \\vec{n})}{\\iint_{[0, 1]^g} \\prod_{t \\leq s} (p_{ts}')^{\\ell_{ts}} (1 - (p_{ts}'))^{r_{ts} - \\ell_{ts}} \\mathbb{P}(\\textbf{P}' \\;|\\; \\vec{n}) \\; \\text{d}\\textbf{P}'}.$$\n\nFinally, we make the assumption that the values in the probability matrix $\\textbf{P}'$ does not depend on the partitioning into groups $\\vec{n}$. This means that $ \\mathbb{P}(\\textbf{P}' \\;|\\; \\vec{n}) = c$ for some constant $c>0$ for all matrices $\\textbf{P}'$. Hence, we find $$\\mathbb{P}(\\{i, j\\} \\in E \\;|\\; \\vec{n}, G^O) = \\iint_{[0, 1]^g} p_{T_i T_j} \\cdot \\frac{\\prod_{t \\leq s} p_{ts}^{\\ell_{ts}} (1 - p_{ts})^{r_{ts} - \\ell_{ts}} }{\\iint_{[0, 1]^g} \\prod_{t \\leq s} (p_{ts}')^{\\ell_{ts}} (1 - (p_{ts}'))^{r_{ts} - \\ell_{ts}} \\; \\text{d}\\textbf{P}'} \\; \\text{d}\\textbf{P}.$$\n\nSetting $C$ to be a (fixed) normalization constant, computing these integrals finally yields $$\\mathbb{P}(\\{i, j\\} \\in E \\;|\\; \\vec{n}, G^O) = \\frac{1}{C} \\cdot \\frac{\\ell_{T_i T_j} + 1}{r_{T_i T_j} + 2} \\prod_{t \\leq s} \\frac{1}{(r_{ts} + 1) \\binom{r_{ts}}{\\ell_{ts}}}.$$ ","metadata":{}},{"cell_type":"markdown","source":"**Exercise 4.** Look at the final expression after the derivaiton. Write down what is contained in the constant $C$, and explain why it is not needed to specify this number. Also, critically evaluate whether everything we do not \"need\" in the above expression is dumped into the constant $C$.","metadata":{}},{"cell_type":"markdown","source":"### Explanation of the Constant \\( C \\) in the Final Expression\n\nTo understand the final expression better, let's carefully analyze what the constant \\( C \\) represents and why it is not necessary to specify this number.\n\n#### Final Expression Recap\n\nThe final expression we arrived at after performing the integration is:\n\n$\n\\mathbb{P}(\\{i, j\\} \\in E \\;|\\; \\vec{n}, G^O) = \\frac{1}{C} \\cdot \\frac{\\ell_{T_i T_j} + 1}{r_{T_i T_j} + 2} \\prod_{t \\leq s} \\frac{1}{(r_{ts} + 1) \\binom{r_{ts}}{\\ell_{ts}}}.\n$\n\n\n#### What Does \\( C \\) Contain?\n\nThe constant \\( C \\) is a normalization factor that depends on the integration over all possible matrices P. It encompasses all terms that are independent of the specific edge \\( \\{i, j\\} \\). To break it down:\n\n$\nC = \\iint_{[0, 1]^g} \\prod_{t \\leq s} (p_{ts})^{\\ell_{ts}} (1 - p_{ts})^{r_{ts} - \\ell_{ts}} \\; \\text{d}\\textbf{P}.\n$\n\nThis integral represents the denominator in the application of Bayes' rule, where we integrate over all possible values of the matrix P. In other words, \\( C \\) ensures that the total probability distribution over all possible edges is properly normalized. It integrates over the likelihoods of all possible probability matrices P, given the observed graph \\( G^O \\) and the partition \\( \\vec{n} \\).\n\n#### Why Is \\( C \\) Not Needed to Specify?\n\n\\( C \\) is not needed to specify because it is a **normalization constant** that does not affect the relative probabilities. In many applications of Bayesian inference and probabilistic modeling, it is common to work with expressions up to a proportional constant. Here's why:\n\n1. **Relative Probabilities**: When calculating posterior distributions or likelihood ratios, the constant \\( C \\) cancels out. Thus, we often only need the probability up to a proportional constant to make decisions or comparisons.\n\n2. **Integration Over All Possible States**: The integral represented by \\( C \\) involves all possible configurations of the probability matrix P. Computing this directly can be extremely difficult or infeasible in practice. However, for the purposes of inference or deriving relative likelihoods, we do not need the explicit value of C.\n\n3. **Normalization in Probability**: Since C is a normalization factor, it guarantees that the sum (or integral) of all probabilities is 1. In many statistical models, normalization constants are considered implicitly rather than explicitly, as they do not affect the shape of the distribution.\n\n4. **Irrelevance in Practical Computation**: In practical terms, especially for large models, explicitly calculating C is often avoided. Instead, algorithms (e.g., Markov Chain Monte Carlo, variational methods) work with unnormalized densities and focus on relative values.\n\n#### Is Everything We Do Not \"Need\" Dumped into C?\n\nYes, everything that is not directly relevant to the calculation of the probability of a specific edge \\( \\{i, j\\} \\) given the observed graph \\( G^O \\) is absorbed into the constant C. This includes:\n\n- The **normalization over all possible matrices** P, which accounts for the total space of possible configurations.\n- Any terms that **do not depend on the specific edge** \\( \\{i, j\\} \\), but rather on the entire configuration of the graph.\n\nThe constant \\( C \\) is designed to capture all the integrals and summations that are computationally burdensome or irrelevant to the direct calculation of conditional probabilities. Thus, the approach simplifies the problem to manageable computations, leveraging the fact that we only care about ratios or relative magnitudes, not the exact probabilities.\n\n### Conclusion\n\nThe constant C contains all terms needed to normalize the probability distribution over the entire space of possible configurations of P. It is not needed in practical computations since it does not affect relative probabilities or likelihoods. Everything that is not explicitly necessary for computing the desired conditional probability \\( \\mathbb{P}(\\{i, j\\} \\in E \\;|\\; \\vec{n}, G^O) \\) is indeed absorbed into C. This is a common technique in probabilistic modeling to focus on the essential computations without getting bogged down by intractable integrals.","metadata":{}},{"cell_type":"markdown","source":"**Exercise 5.** Load the network ``link_prediction_ML1.gz``. It is given that this network was generated through a stochastic block model where the first $50$ vertices are of type $1$, the next $30$ of type $2$ and the final $20$ of type $3$. Write code to perform link prediciton on this network using the SBM ML method given the above likelihood function. Given that we removed link $\\{14, 48\\}$, are the results as expected?","metadata":{}},{"cell_type":"code","source":"import networkx as nx\nimport numpy as np\nimport scipy.special as sp\n\n# Load the network\nG = nx.read_edgelist('/kaggle/input/network-statistics-for-data-science/link_prediction_ML1/link_prediction_ML1', nodetype=int)\n\n# Define the partition sizes for the SBM\nn = [50, 30, 20]  # Sizes of each group\ntype_labels = [1] * n[0] + [2] * n[1] + [3] * n[2]  # Type labels for nodes\n\n# Initialize dictionaries to store observed and possible links between types\nl_counts = np.zeros((3, 3))  # Observed links (ell_ts)\nr_counts = np.zeros((3, 3))  # Possible links (r_ts)\n\n# Compute the observed and possible link counts\nfor i in range(len(n)):\n    for j in range(i, len(n)):\n        # Get nodes of type i and j\n        nodes_i = [node for node in G.nodes() if type_labels[node] == i + 1]\n        nodes_j = [node for node in G.nodes() if type_labels[node] == j + 1]\n        \n        # Possible links (r_ts) between nodes of type i and j\n        r_counts[i, j] = len(nodes_i) * len(nodes_j) if i != j else len(nodes_i) * (len(nodes_i) - 1) / 2\n        \n        # Observed links (ell_ts) between nodes of type i and j\n        if i == j:\n            l_counts[i, j] = len([edge for edge in G.edges(nodes_i) if type_labels[edge[0]] == i + 1 and type_labels[edge[1]] == i + 1]) / 2\n        else:\n            l_counts[i, j] = len([edge for edge in G.edges(nodes_i) if type_labels[edge[1]] == j + 1])\n\n# Function to calculate the likelihood of a link existing\ndef compute_link_likelihood(l_counts, r_counts, i, j):\n    Ti = type_labels[i] - 1  # Type of node i\n    Tj = type_labels[j] - 1  # Type of node j\n    ell_ts = l_counts[Ti, Tj]  # Observed links of types Ti, Tj\n    r_ts = r_counts[Ti, Tj]  # Possible links of types Ti, Tj\n    \n    # Compute the probability for the link {i, j} using the likelihood formula\n    prob = (ell_ts + 1) / (r_ts + 2)\n    \n    return prob\n\n# Predict missing links using the likelihood method\nmissing_link_likelihoods = {}\n\nfor i in range(len(G.nodes())):\n    for j in range(i + 1, len(G.nodes())):\n        if not G.has_edge(i, j):  # Only consider missing links\n            likelihood = compute_link_likelihood(l_counts, r_counts, i, j)\n            missing_link_likelihoods[(i, j)] = likelihood\n\n# Sort missing links by likelihood\nsorted_links = sorted(missing_link_likelihoods.items(), key=lambda x: x[1], reverse=True)\n\n# Check the likelihood of the removed link {14, 48}\nremoved_link = (14, 48)\nremoved_link_likelihood = compute_link_likelihood(l_counts, r_counts, removed_link[0], removed_link[1])\n\nprint(f\"Likelihood of removed link {removed_link}: {removed_link_likelihood:.6f}\")\n\n# Check if the removed link is among the top predicted missing links\nrank = next((rank for rank, link in enumerate(sorted_links) if link[0] == removed_link), None)\n\nprint(f\"Rank of removed link {removed_link} in predicted missing links: {rank + 1 if rank is not None else 'Not in top predictions'}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T08:37:58.635609Z","iopub.execute_input":"2024-09-16T08:37:58.636102Z","iopub.status.idle":"2024-09-16T08:37:58.697920Z","shell.execute_reply.started":"2024-09-16T08:37:58.636048Z","shell.execute_reply":"2024-09-16T08:37:58.696188Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Likelihood of removed link (14, 48): 0.239609\nRank of removed link (14, 48) in predicted missing links: 328\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Although the above derivation was already quite involved, you have probably noticed from Exercise 5 that giving away $\\vec{n}$ and the exact location of each vertex-type is too much. There is not enough left to make the SBM ML link prediction method useful. Luckily (or to be fair sadly) we often also do not know (1) how many vertex-types there are, and (2) where these types are located. Thus, in reality the likelyhood will be given by $\\mathbb{P}(\\{i, j\\} \\in E \\;|\\; G^O)$. We can repeat the above caclulations to show that this will yields the following likelihood function for the stochastic block model: $$\\mathbb{P}(\\{i, j\\} \\in E \\;|\\; G^O) =  \\frac{1}{C} \\sum_{\\textbf{T} \\in \\mathcal{T}} \\frac{\\ell_{T_i T_j} + 1}{r_{T_i T_j} + 2} \\prod_{t \\leq s} \\frac{1}{(r_{ts} + 1) \\binom{r_{ts}}{\\ell_{ts}}}.$$ Here, the newly added sum forces us to compute the previous likelihood for all possible assignments of types to the vertices. So, $\\mathcal{T}$ can be seen as the set of all type-assignments to the vertices in the graph, and $\\textbf{T}$ can be seen as a signle sequence of vertex-types sampled from $\\mathcal{T}$; we also call $\\textbf{T}$ a **partition**. This is the likelihood function we need to make the SBM ML based method work. The question now becomes: how do we implement it?","metadata":{}},{"cell_type":"markdown","source":"**Exercise 6.** Look at the likelihood expression above, and answer the following questions. Write down your reasoning behind each answer.\n- What is the value of $C$ in the likelihood expression above?\n- Can the product in the likelihood expression above be taken out of the sum?\n- Does the above likelihood function use global information about the graph $G^O$ or only local information around vertices $i$ and $j$?\n- How many terms does the sum have? \n- How fast does the number of terms in the sum grow as $G^O$ has more vertices?","metadata":{}},{"cell_type":"markdown","source":"To analyze the likelihood expression provided in the context of stochastic block models (SBMs), let's answer each question step-by-step:\n\n### Likelihood Function for Stochastic Block Model\n\nThe given likelihood function for predicting the existence of an edge \\(\\{i, j\\}\\) in a graph \\(G^O\\) is:\n\n\\[\n\\mathbb{P}(\\{i, j\\} \\in E \\;|\\; G^O) =  \\frac{1}{C} \\sum_{\\textbf{T} \\in \\mathcal{T}} \\frac{\\ell_{T_i T_j} + 1}{r_{T_i T_j} + 2} \\prod_{t \\leq s} \\frac{1}{(r_{ts} + 1) \\binom{r_{ts}}{\\ell_{ts}}}.\n\\]\n\nHere:\n- \\(\\mathcal{T}\\) is the set of all possible type-assignments (partitions) to the vertices in the graph.\n- \\(\\textbf{T}\\) is a single sequence of vertex types sampled from \\(\\mathcal{T}\\).\n- \\(\\ell_{T_i T_j}\\) is the number of observed links between types \\(T_i\\) and \\(T_j\\).\n- \\(r_{T_i T_j}\\) is the maximum possible number of links between types \\(T_i\\) and \\(T_j\\).\n\nNow, let's analyze the questions:\n\n### Question 1: What is the value of \\(C\\) in the likelihood expression above?\n\nThe constant \\(C\\) in the likelihood expression is a normalization constant. Its role is to ensure that the probability values computed from the likelihood function sum to 1 over all possible configurations. \n\nGiven the sum over all possible type assignments \\(\\mathcal{T}\\), the value of \\(C\\) can be expressed as:\n\n\\[\nC = \\sum_{\\textbf{T} \\in \\mathcal{T}} \\left( \\frac{\\ell_{T_i T_j} + 1}{r_{T_i T_j} + 2} \\prod_{t \\leq s} \\frac{1}{(r_{ts} + 1) \\binom{r_{ts}}{\\ell_{ts}}} \\right).\n\\]\n\nThis normalization constant \\(C\\) ensures that the total probability across all possible edges sums to 1. However, in the context of link prediction, knowing the exact value of \\(C\\) is generally not needed if we only care about the relative likelihood of different edges.\n\n### Question 2: Can the product in the likelihood expression above be taken out of the sum?\n\nNo, the product in the likelihood expression cannot be taken out of the sum. This is because the product \\(\\prod_{t \\leq s} \\frac{1}{(r_{ts} + 1) \\binom{r_{ts}}{\\ell_{ts}}}\\) depends on the specific partition \\(\\textbf{T}\\) chosen from the set \\(\\mathcal{T}\\). Each partition \\(\\textbf{T}\\) gives a different set of \\(\\ell_{ts}\\) values (observed links) and, consequently, different terms in the product. Therefore, the product must remain inside the sum.\n\n### Question 3: Does the above likelihood function use global information about the graph \\(G^O\\) or only local information around vertices \\(i\\) and \\(j\\)?\n\nThe likelihood function uses **global information** about the graph \\(G^O\\). \n\n- The likelihood function sums over all possible type assignments \\(\\textbf{T}\\) for the entire graph. This requires considering the overall structure of the graph and the distribution of links between all groups of nodes.\n- The terms \\(\\ell_{ts}\\) and \\(r_{ts}\\) represent the number of observed links and possible links between types \\(t\\) and \\(s\\) for the entire graph, not just around vertices \\(i\\) and \\(j\\). Thus, the likelihood function is inherently global.\n\n### Question 4: How many terms does the sum have?\n\nThe number of terms in the sum corresponds to the number of possible type assignments \\(\\textbf{T}\\) in the set \\(\\mathcal{T}\\). If there are \\(N\\) vertices in the graph and \\(K\\) possible types (blocks), the number of terms in the sum is given by:\n\n\\[\n|\\mathcal{T}| = K^N.\n\\]\n\nThis is because each vertex can be assigned to any of the \\(K\\) types, leading to \\(K^N\\) possible type assignments.\n\n### Question 5: How fast does the number of terms in the sum grow as \\(G^O\\) has more vertices?\n\nThe number of terms in the sum grows **exponentially** with the number of vertices \\(N\\). Specifically, it grows as \\(K^N\\), where \\(K\\) is the number of types (blocks). If \\(K\\) is fixed, the number of terms increases very rapidly as the number of vertices \\(N\\) increases. This exponential growth makes the computation intractable for large graphs, necessitating approximations or heuristic methods to efficiently estimate the likelihood.\n\n### Conclusion\n\nTo summarize:\n\n- \\(C\\) is a normalization constant that ensures the probabilities sum to 1.\n- The product cannot be taken out of the sum because it depends on the specific partition.\n- The likelihood function uses global information about the entire graph.\n- The number of terms in the sum is \\(K^N\\).\n- The number of terms grows exponentially with the number of vertices, making direct computation infeasible for large graphs.","metadata":{}},{"cell_type":"markdown","source":"## Approximating the likelihood function of the SBM","metadata":{}},{"cell_type":"markdown","source":"We now know what the likelihood function of the SBM maximum likelihood method for link prediction looks like, so now we only have to compute this likelihood and we have found the reliability of each vertex. However, as we have seen in Exercise 6, computing the likelihood function is cumbersome and computationally unfeasible if the number of vertices in $G^O$ grows. Thus, we need to find a way to approximate the likelihood function. Since there are too many options to choose from, a first idea we can try, is fixing the number of vertex-types present in the graph and choosing partitions of vertices into groups uniformly at random. You then compute the sum of likelihoods over only those vertex types assignments you have sampled. \n\nIn the rest of this workshop we will consider the (real-life) dataset in ``link_prediction_ML2.gz``. This dataset depicts books about US politics published around the 2004 election. An edge between two books means that the books have often been purchased together on \"amazon.com\". We have removed edge $\\{40, 44\\}$. ","metadata":{}},{"cell_type":"markdown","source":"**Exercise 7$\\star$.** Think about the nature of the network ``link_prediction_ML2.gz``. How many vertex-types do you think there are? Use this knowledge to approximate the likelihood function, and do link prediciton using the SBM ML method on the dataset. How well does the method perform?\n\n*Hint: You can use the skeleton below to guide you*","metadata":{}},{"cell_type":"code","source":"import networkx as nx\nimport numpy as np\nimport math\nimport random\n\ndef twoClassSBMLinkPrediction(Adj, trials=100, noTypes=2):\n    \"\"\"\n    Performs SBM link prediction by randomly sampling partitions with a given number of types.\n    \n    Parameters\n    ----------\n    Adj : Symmetric np.array() with {0, 1} entries;\n        Adjacency matrix of the input graph.\n    trials : int, optional;\n        The number of partitions we sample. The default is 100.\n    noTypes : int, optional;\n        The number of vertex-types we assume. The default is 2.\n    \n    Returns\n    -------\n    reliability : np.array() with float entries;\n        A matrix with the reliabilities of all edges.  \n    \"\"\"\n    n = Adj.shape[0]  # Number of vertices\n    pReliability = np.zeros((trials, n, n))  # To store edge reliabilities over all trials\n\n    for trial in range(trials):\n        # Step 1: Sample a random partition (i.e., assign vertices to one of the noTypes groups)\n        # Randomly assign each vertex to a type (community)\n        partition = np.random.randint(0, noTypes, size=n)\n\n        # Step 2: Compute the number of vertices in each type\n        type_counts = np.zeros(noTypes)\n        for t in range(noTypes):\n            type_counts[t] = np.sum(partition == t)\n\n        # Initialize block-wise probabilities (edge existence between types)\n        block_probabilities = np.zeros((noTypes, noTypes))\n\n        # Step 3: Compute edge-dependent part of the likelihood function for each edge\n        for i in range(n):\n            for j in range(i + 1, n):\n                type_i = partition[i]\n                type_j = partition[j]\n\n                # Count edges between types to estimate block probabilities\n                if Adj[i, j] == 1:\n                    block_probabilities[type_i, type_j] += 1\n                    if type_i != type_j:\n                        block_probabilities[type_j, type_i] += 1  # Symmetric case\n\n        # Normalize block probabilities (divide by the number of possible edges between types)\n        for t1 in range(noTypes):\n            for t2 in range(noTypes):\n                if t1 == t2:\n                    possible_edges = type_counts[t1] * (type_counts[t1] - 1) / 2\n                else:\n                    possible_edges = type_counts[t1] * type_counts[t2]\n\n                if possible_edges > 0:\n                    block_probabilities[t1, t2] /= possible_edges\n\n        # Step 4: Use the block probabilities to predict edges (reliability)\n        for i in range(n):\n            for j in range(i + 1, n):\n                type_i = partition[i]\n                type_j = partition[j]\n                edge_prob = block_probabilities[type_i, type_j]\n\n                # Store the reliability of this edge in the current trial\n                pReliability[trial, i, j] = edge_prob\n                pReliability[trial, j, i] = edge_prob  # Symmetry\n\n    # Step 5: Sum over all computed reliabilities across partitions\n    reliability = np.mean(pReliability, axis=0)  # Average over all trials\n\n    return reliability\n\n# Load the edgelist into a NetworkX graph\nAdj = \"/kaggle/input/network-statistics-for-data-science/link_prediction_ML2/link_prediction_ML2\"\nG_adj = nx.read_edgelist(Adj, nodetype=int)\n\n# Convert the graph to a NumPy adjacency matrix\nAdj_matrix = nx.to_numpy_array(G_adj)\n\n# Perform SBM link prediction\nreliability_matrix = twoClassSBMLinkPrediction(Adj_matrix)\n\n# Output the reliability matrix\nprint(reliability_matrix)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T08:46:59.438420Z","iopub.execute_input":"2024-09-16T08:46:59.438907Z","iopub.status.idle":"2024-09-16T08:47:00.986846Z","shell.execute_reply.started":"2024-09-16T08:46:59.438866Z","shell.execute_reply":"2024-09-16T08:47:00.985552Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[[0.         0.07968943 0.07941604 ... 0.08108977 0.07964128 0.08018139]\n [0.07968943 0.         0.079454   ... 0.08015862 0.07967    0.07976239]\n [0.07941604 0.079454   0.         ... 0.08018828 0.08051544 0.07965716]\n ...\n [0.08108977 0.08015862 0.08018828 ... 0.         0.0805395  0.07976555]\n [0.07964128 0.07967    0.08051544 ... 0.0805395  0.         0.07998468]\n [0.08018139 0.07976239 0.07965716 ... 0.07976555 0.07998468 0.        ]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"On of the problems you might have encountered in Exercise 7, is that the term $$f(\\textbf{T}) = \\prod_{t \\leq s} \\frac{1}{(r_{ts} + 1) \\binom{r_{ts}}{\\ell_{ts}}}$$ in the likelihood function is quite dominating. Hence, it is not very useful to saple partitions uniformly at random, since many of them will not contribute much to the final result anyways. We need to find a way to sample *relevant* partitions only. To do this, we apply the following idea:\n1. Start with a random partition $\\textbf{T}_{\\text{old}}$.\n2. Change the type of one of the vertices in $\\textbf{T}_{\\text{old}}$ randomly to create $\\textbf{T}_{\\text{new}}$.\n3. Compute $f(\\textbf{T}_{\\text{new}})$. We accept the change in Step 2 (and set $\\textbf{T}_{\\text{old}}$ to be equal to $\\textbf{T}_{\\text{new}}$) if:\n    * The value $f(\\textbf{T}_{\\text{new}})$ is larger than $f(\\textbf{T}_{\\text{old}})$, or\n    * If $f(\\textbf{T}_{\\text{new}})$ is smaller than $f(\\textbf{T}_{\\text{old}})$, then accept the change only with probability $f(\\textbf{T}_{\\text{new}}) / f(\\textbf{T}_{\\text{old}})$.\n4. After a large enough number of accepted changes, the value of $f(\\textbf{T}_{\\text{new}})$ should have converged to the value \"important\" partitions take. Now, we can start to sample partitions, and compute the likelihoods of them.\n5. Repteat Steps 2 and 3 above until a fixed number of partition changes have been accepted.\n6. Compute the likelihood of the partition you have obtained after Step 5 for all edges and store it.\n7. Reteat Steps 5 and 6 until you have enough samples of the likelihood function. Sum over all obtained partition to find the reliability of each edge.\n\nAnother problem you might have faced in Exercise 7, is that the value $C$ was hard to guess and that the likelihood function was numerically unstable. There are some practicalities to keep in mind with regards to these issues:\n* Working with $f(\\textbf{T}_{\\text{new}})$ directly is numerically unstable if you have many possible vertex-types. You might want to consider using only $\\log(f(\\textbf{T}_{\\text{new}}))$.\n* You need to choose the normalization constant $C$ in the likelihood function cleverly. It might be smart to save the \"converged\" value of $f(\\textbf{T}_{\\text{old}})$ and use that is your value of $C$, since you expect all subsequent evaluations of $f$ to be close to this value.","metadata":{}},{"cell_type":"markdown","source":"**Exercise 8.** Implement the above idea to do link prediction with the likelihood based stochastic block model method on ``link_prediction_ML2.gz``. How does it perform? To help you get started, below you will find a function that executes Step 1 through 4. Check this function thoroughly, so you understand what it does; there might still be tiny mistakes in it.\n\n*Hint: You may fix the number of vertex-types, but you may also keep it unknown. The idea of the algorithm works in both settings, but the implementation is slightly simpler if you know the number of vertex-types.*","metadata":{}},{"cell_type":"code","source":"import networkx as nx\nimport numpy as np\nimport math\nimport random\n\n\ndef initialConfigF(Adj, noTypes=100, trainSteps=1000):\n    \"\"\"\n    Performs initial convergence of f(P) in the Metropolis algorithm.\n\n    Parameters\n    ----------\n    Adj : Symmetric np.array() with {0, 1} entries;\n        Adjacency matrix of the input graph.\n    noTypes : int, optional;\n        The number of communities we assume. The default is 100.\n    trainSteps : int, optional;\n        The number of iterative steps taken to converge f(P). The default is 1000.\n\n    Returns\n    -------\n    Fval : float;\n        The final (converged) value of log(f(P)). The logarithm is given to\n        keep the algorithm numerically stable.\n    oldP : np.array() with int entries;\n        The partition P that produced f(P).\n    \"\"\"\n    # Initial training for f\n    Fval = - 10**10\n    oldP = np.random.choice(noTypes, Adj.shape[0])  # Choose a random partition\n\n    for step in range(trainSteps):\n        fail = True\n\n        # Keep trying new partition until you've found one that is accepted\n        while fail:\n            # Create new P\n            index = np.random.randint(Adj.shape[0])\n            newP = oldP.copy()\n            newP[index] = np.random.choice(np.delete(np.arange(noTypes), oldP[index]))\n\n            # Compute the logarithm of f(P) [for numerical stability]\n            n = np.histogram(newP, bins=np.arange(noTypes + 1))[0]\n            normconst = 0\n            for k, val1 in enumerate(n):\n                for l, val2 in enumerate(n[k:]):\n                    l = k + l  # Shift the index l to the correct value\n                    rDummy = val1 * val2\n                    c = Adj[newP == k, :]\n                    d = c[:, newP == l]\n                    ellDummy = np.sum(d)\n                    if k == l:\n                        rDummy = (val1 - 1) * val1 / 2\n                        ellDummy = ellDummy / 2\n                    if rDummy <= 50 or ellDummy == 0:\n                        normconst += -np.log(rDummy + 1) - np.log(math.comb(int(rDummy), int(ellDummy)))\n                    else:\n                        # Approximation of logarithm of binomial coefficient\n                        normconst += -np.log(rDummy + 1) - (rDummy * np.log(rDummy) - ellDummy * np.log(ellDummy) - (rDummy - ellDummy) * np.log(rDummy - ellDummy))\n\n            # Test whether we accept the change, and if accepted: update\n            if np.random.binomial(1, min([1., np.exp(normconst - Fval)])) == 1:\n                Fval = normconst\n                oldP = newP.copy()\n                fail = False\n\n    return Fval, oldP\n\n\ndef sbmLinkPrediction(Adj, noTypes=100, trainSteps=1000, testSet=None):\n    \"\"\"\n    Performs link prediction using SBM and returns the predicted probabilities.\n\n    Parameters\n    ----------\n    Adj : np.array\n        Adjacency matrix of the graph.\n    noTypes : int\n        The number of community types.\n    trainSteps : int\n        The number of steps for partition convergence.\n    testSet : list of tuples, optional\n        List of node pairs in the test set to predict links for.\n\n    Returns\n    -------\n    reliability : np.array\n        Reliability matrix containing link prediction scores.\n    \"\"\"\n    # Get initial partition using the Metropolis algorithm\n    Fval, partition = initialConfigF(Adj, noTypes=noTypes, trainSteps=trainSteps)\n\n    # Create block probability matrix\n    n = len(partition)\n    block_probabilities = np.zeros((noTypes, noTypes))\n    type_counts = np.zeros(noTypes)\n\n    # Count the number of edges between each pair of communities\n    for i in range(n):\n        for j in range(i + 1, n):\n            type_i = partition[i]\n            type_j = partition[j]\n            if Adj[i, j] == 1:\n                block_probabilities[type_i, type_j] += 1\n                if type_i != type_j:\n                    block_probabilities[type_j, type_i] += 1\n\n    # Normalize block probabilities based on number of possible edges\n    for i in range(noTypes):\n        for j in range(noTypes):\n            if i == j:\n                possible_edges = type_counts[i] * (type_counts[i] - 1) / 2\n            else:\n                possible_edges = type_counts[i] * type_counts[j]\n            if possible_edges > 0:\n                block_probabilities[i, j] /= possible_edges\n\n    # Predict links for the test set based on block probabilities\n    reliability = np.zeros((n, n))\n    for i in range(n):\n        for j in range(i + 1, n):\n            type_i = partition[i]\n            type_j = partition[j]\n            reliability[i, j] = block_probabilities[type_i, type_j]\n            reliability[j, i] = reliability[i, j]\n\n    return reliability\n\n\n# Example usage:\n# Load the adjacency matrix from the gzipped file\n#Adj = np.loadtxt(\"link_prediction_ML2.gz\")\nAdj = \"/kaggle/input/network-statistics-for-data-science/link_prediction_ML2/link_prediction_ML2\"\nG_adj = nx.read_edgelist(Adj, nodetype=int)\nAdj_matrix = nx.to_numpy_array(G_adj)\n\n# # Perform SBM link prediction\n# reliability_matrix = sbmLinkPrediction(Adj_matrix)\n# print(reliability_matrix)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T08:58:53.557999Z","iopub.execute_input":"2024-09-16T08:58:53.558911Z","iopub.status.idle":"2024-09-16T08:58:53.609185Z","shell.execute_reply.started":"2024-09-16T08:58:53.558860Z","shell.execute_reply":"2024-09-16T08:58:53.607990Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Validation","metadata":{}},{"cell_type":"markdown","source":"In certain exercises you were asked to assess the performance of a link prediction algorithm. You have probably found out that this is not an easy task. In general, validation for link prediction is a difficult talk, since we do not know that the actual network looks like. Usually this lack of a ground truth is mediated by using **cross-validation** methods. The idea is to remove extra edges from a network, and using the link prediction method to assess how well these extra edges are retrieved. By doing this multiple times, and averaging over the results you can obtain a score that tells you how well the link prediction method performs on a given problem.","metadata":{}},{"cell_type":"markdown","source":"**Exercise 9.** The code below implements a specific cross-validation method for the Jaccard index (this is a similarity based link prediction method). Find out what the code does, and add comments to explain the procedure. Then, adapt it to the SBM maximum likelihood link prediction method, and use it to test the performance of this algorithm.","metadata":{}},{"cell_type":"code","source":"def leaveOneOutJaccard(G):\n    AUC = 0\n    for e in G.edges:\n        Gprobed = G.copy()\n        Gprobed.remove_edge(e[0],e[1])\n        target = nx.jaccard_coefficient(Gprobed, [e])\n        for _, _, p in target:\n            reliabilityTarget = p\n        \n        reliability = nx.jaccard_coefficient(Gprobed)\n        better = 0\n        same = 0\n        total = 0\n        for _, _, p in reliability:\n            if p < reliabilityTarget:\n                better += 1\n            elif p == reliabilityTarget:\n                same += 1\n            total += 1\n        \n        AUC += (0.5 * same + better) / total\n    return AUC / len(G.edges)\n\n\ndef leaveOneOutSBM(Adj, G, noTypes=2, trainSteps=1000):\n    \"\"\"\n    Leave-One-Out Cross-Validation for SBM Maximum Likelihood Link Prediction\n\n    Parameters:\n    -----------\n    Adj : np.array\n        Adjacency matrix of the graph.\n    G : networkx.Graph\n        The graph to test on.\n    noTypes : int\n        Number of communities for SBM.\n    trainSteps : int\n        Number of steps for the Metropolis algorithm in SBM.\n    \n    Returns:\n    --------\n    AUC : float\n        Area Under the Curve (AUC) score for SBM link prediction.\n    \"\"\"\n    AUC = 0  # Initialize AUC score to 0\n    \n    # Loop over all edges in the graph for leave-one-out cross-validation\n    for e in G.edges:\n        # Create a probed version of the adjacency matrix with the edge removed\n        Gprobed = G.copy()\n        Gprobed.remove_edge(e[0], e[1])\n        \n        # Get the new adjacency matrix after the edge removal\n        Adj_probed = nx.to_numpy_array(Gprobed)\n        \n        # Perform SBM link prediction on the modified graph\n        reliability_matrix = sbmLinkPrediction(Adj_probed, noTypes=noTypes, trainSteps=trainSteps)\n        \n        # Store the reliability score for the target (removed) edge\n        reliabilityTarget = reliability_matrix[e[0], e[1]]\n        \n        # Compare the reliability score of all edges to the target edge\n        better = 0\n        same = 0\n        total = 0\n        \n        # Iterate over all node pairs and compare their reliability scores to the target edge\n        for i in range(Adj.shape[0]):\n            for j in range(i + 1, Adj.shape[0]):\n                if Gprobed.has_edge(i, j) or (i, j) == e:\n                    continue  # Skip pairs that are already connected, and skip the removed edge\n                reliability = reliability_matrix[i, j]\n                if reliability < reliabilityTarget:\n                    better += 1  # Count if the score is lower than the target score\n                elif reliability == reliabilityTarget:\n                    same += 1  # Count if the score is equal to the target score\n                total += 1  # Count total comparisons\n        \n        # Update the AUC based on comparisons made\n        AUC += (0.5 * same + better) / total  # Weight same scores as 0.5, better scores as 1\n    \n    # Return the average AUC score over all edges\n    return AUC / len(G.edges)\n\n\n# Load the adjacency matrix and create a graph\nAdj = \"/kaggle/input/network-statistics-for-data-science/link_prediction_ML2/link_prediction_ML2\"\nG_adj = nx.read_edgelist(Adj, nodetype=int)\nAdj_matrix = nx.to_numpy_array(G_adj)\n\n# Perform leave-one-out cross-validation for SBM link prediction\nAUC_SBM = leaveOneOutSBM(Adj_matrix, G_adj, noTypes=3, trainSteps=1000)\n\nprint(f\"AUC Score for SBM Link Prediction: {AUC_SBM}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T09:05:24.380167Z","iopub.execute_input":"2024-09-16T09:05:24.380653Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/4172621528.py:62: RuntimeWarning: overflow encountered in exp\n  if np.random.binomial(1, min([1., np.exp(normconst - Fval)])) == 1:\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In this workshop you have now tried the stochastic block model maximum likelihood method for link prediction on both real and synthetic data. You should now have noticed the following:\n- Link prediction is a difficult task, and in practice it is almost impossible to retrieve the missing links exactly.\n- Maximum likelihood methods are often difficult to implement and by the nature of the likelihood function numerically unstable.\n- The performance of maximum likelihood methods heavily depends on how suitible your underlying null model is for the problem.\n- You need to understand the stochastic structure of the underlying null model well if you want to use it for link predictoin.\n- It is not easy to validate and assess performance of link prediction algorithms (more on this in a later lecture).","metadata":{}}]}