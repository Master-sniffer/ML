{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"toc_visible":true},"accelerator":"GPU","gpuClass":"standard","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/vlamen/tue-deeplearning/blob/main/tutorials/P2.2_U_net_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"code","source":"%matplotlib inline","metadata":{"id":"emH5vNyyhlFv","execution":{"iopub.status.busy":"2024-05-03T12:09:36.545326Z","iopub.execute_input":"2024-05-03T12:09:36.545695Z","iopub.status.idle":"2024-05-03T12:09:36.577760Z","shell.execute_reply.started":"2024-05-03T12:09:36.545663Z","shell.execute_reply":"2024-05-03T12:09:36.576927Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"\n#  Tutorial Contents\n\nIn this tutorial,you will learn:\n\n(1) The U-Net structure\n\n(2) How to build a U-net from scratch\n\n(3) How to refactor U-net to do image segmentation","metadata":{"id":"sXomwmFKhlFw"}},{"cell_type":"markdown","source":"\n#  1. Understanding U-Net structure\n","metadata":{"id":"DUk7FhZBhlFx"}},{"cell_type":"markdown","source":"The U-Net is a convolutional neural network architecture that is designed for fast and precise segmentation of images. it is one of the most popular end-to-end architectures in the field of semantic segmentation.","metadata":{"id":"h6g7_c38hlFx"}},{"cell_type":"markdown","source":":<center><img src=\"https://raw.githubusercontent.com/vlamen/tue-deeplearning/main/img/unet.png\" alt=\"Unet architecture\" width=\"500\"/></center>","metadata":{"id":"6cbvIPTshlFx"}},{"cell_type":"markdown","source":"From the paper [[1]](https://arxiv.org/pdf/1505.04597.pdf,),\n\nThe network architecture is illustrated in Fig-1. It consists of a contracting path (left side), an expanding path (right side) and skip connections (horizontal connections depicted in gray).\n\n\n**[1]U-Net: Convolutional Networks for Biomedical Image Segmentation**\n","metadata":{"id":"3vqJ3s7khlFy"}},{"cell_type":"markdown","source":"The contracting path follows the typical architecture of a convolutional network. Its goal is to develop a global representation of the image as a series of contractions of larger and larger neighborhoods. The contracting path captures features at different scales of the images by using a traditional stack of convolutional and max pooling layers. Concretely speaking, a block in the encoder consists of the repeated use of two convolutional layers (k=3, s=1), each followed by a non-linearity layer, and a max-pooling layer (k=2, s=2). For every convolution block and its associated max pooling operation, the number of feature maps is doubled to ensure that the network can learn the complex structures effectively.","metadata":{"id":"TcH_huFuhlFy"}},{"cell_type":"markdown","source":"The expanding path is symmetric counterpart to the contraction. Its role is to distribute and mix the information that comes from the global features and the local details given by the skip connections. This path uses transposed convolutions. This type of convolutional layer is an up-sampling method with trainable parameters and performs the reverse of (down)pooling layers such as the max pool. Each convolution block is followed by an up-convolutional layer. The number of feature maps is halved in every block. Because recreating a segmentation mask from a small feature map is a rather difficult task for the network, the output after every up-convolutional layer is appended by the feature maps of the corresponding encoder block. The feature maps of the encoder layer are cropped if the dimensions exceed the one of the corresponding decoder layers.","metadata":{"id":"bs4Bek8khlFz"}},{"cell_type":"markdown","source":"# 2. Building a U-net from scratch","metadata":{"id":"IOuU5kPzhlFz"}},{"cell_type":"markdown","source":"Let’s start to now turn this simple structure to PyTorch Code","metadata":{"id":"GBGIyhAMhlFz"}},{"cell_type":"markdown","source":"## Building Conv Block","metadata":{"id":"OGaEATa1hlF0"}},{"cell_type":"markdown","source":"We will need two Conv2D operations with ReLU activation in both contracting path  and expanding path. The Convolution operations have kernel size of 3, and no padding. Therefore, the output feature map doesn’t have the same Height and Width as the input feature map.  Batch normalization is introduced to increase the speed at which networks train and make weights easier to initialize.","metadata":{"id":"txz7ftUfhlF0"}},{"cell_type":"markdown","source":"From the paper [[1]](https://arxiv.org/pdf/1505.04597.pdf,):\n\n\n*The contractive path consists of the repeated application of two 3x3 convolutions (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a 2x2 max pooling operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels.*\n\n\nLet’s write Block in code.","metadata":{"id":"GW0l2OKHhlF0"}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\nclass  Block(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)","metadata":{"id":"Hb5iPOi5hlF1","execution":{"iopub.status.busy":"2024-05-03T12:09:36.696346Z","iopub.execute_input":"2024-05-03T12:09:36.697058Z","iopub.status.idle":"2024-05-03T12:09:40.103571Z","shell.execute_reply.started":"2024-05-03T12:09:36.697023Z","shell.execute_reply":"2024-05-03T12:09:40.102466Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"That’s simple - two convolution operations, one that doubles the number of channels from `in_channels` to `out_channels` and another that goes from `out_channels` to `out_channels`. Both are 2D convolutions with kernel size 3 and no padding as mentioned in the paper followed by `BatchNorm2d` and `ReLU` activation.\n\nLet’s make sure this works.","metadata":{"id":"VthoFEG2hlF1"}},{"cell_type":"code","source":"enc_block = Block(1, 64)\nx         = torch.randn(1, 1, 572, 572)\nenc_block(x).shape\n","metadata":{"id":"3jDasLirhlF1","execution":{"iopub.status.busy":"2024-05-03T12:09:40.106003Z","iopub.execute_input":"2024-05-03T12:09:40.106574Z","iopub.status.idle":"2024-05-03T12:09:41.109912Z","shell.execute_reply.started":"2024-05-03T12:09:40.106534Z","shell.execute_reply":"2024-05-03T12:09:41.108913Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 64, 568, 568])"},"metadata":{}}]},{"cell_type":"markdown","source":"So this is looking good, the output size matches that in `fig-1` top-left. Given an input image with shape `1x572x572` the output is of shape `64x568x568`.","metadata":{"id":"mBgQMWMbhlF2"}},{"cell_type":"markdown","source":"## The Encoder","metadata":{"id":"KIaLRCXMhlF2"}},{"cell_type":"markdown","source":"Now we are ready to implement the Encoder. The Encoder is the contractive path of the U-Net Architecture.\n\nSo far we have implemented the convolution operations but not the downsampling part. As mentioned in the paper:\n\n*Each block is followed by a 2x2 max pooling operation with stride 2 for downsampling*\n\n\nSo that’s all we need to do, we need add `MaxPooling` operation  that is performed between two Block operations.","metadata":{"id":"KlPeWkhFhlF2"}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, chs=(3,64,128,256,512,1024)):\n        super().__init__()\n        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n        self.pool       = nn.MaxPool2d(2)\n\n    def forward(self, x):\n        ftrs = []\n        for block in self.enc_blocks:\n            x = block(x)\n            ftrs.append(x)\n            x = self.pool(x)\n        return ftrs\n","metadata":{"id":"6HcxnFFehlF2","execution":{"iopub.status.busy":"2024-05-03T12:09:41.111172Z","iopub.execute_input":"2024-05-03T12:09:41.111487Z","iopub.status.idle":"2024-05-03T12:09:41.121608Z","shell.execute_reply.started":"2024-05-03T12:09:41.111461Z","shell.execute_reply":"2024-05-03T12:09:41.120507Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"That’s all we have done. On the Encoder side, the encoder block or self.enc_blocks is a list of Block operations. Next, we perform the MaxPooling operation to the outputs of every block. Since, we also need to store the outputs of the block, we store them in a list called ftrs and return this list.\n\nLet’s make sure this implementation works.","metadata":{"id":"5XAd2SDehlF2"}},{"cell_type":"code","source":"encoder = Encoder()\n# input image\nx    = torch.randn(1, 3, 572, 572)\nftrs = encoder(x)\nfor ftr in ftrs: print(ftr.shape)","metadata":{"id":"8W9YmRzfhlF3","execution":{"iopub.status.busy":"2024-05-03T12:09:41.124014Z","iopub.execute_input":"2024-05-03T12:09:41.124368Z","iopub.status.idle":"2024-05-03T12:09:43.894491Z","shell.execute_reply.started":"2024-05-03T12:09:41.124340Z","shell.execute_reply":"2024-05-03T12:09:43.893370Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"torch.Size([1, 64, 568, 568])\ntorch.Size([1, 128, 280, 280])\ntorch.Size([1, 256, 136, 136])\ntorch.Size([1, 512, 64, 64])\ntorch.Size([1, 1024, 28, 28])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The output shapes exactly match the shapes mentioned in `fig-1` - so far, so good. Having implemented the `Encoder`, we are now ready to move on the `Decoder`.","metadata":{"id":"tB3W2Tv5hlF3"}},{"cell_type":"markdown","source":"## The Decoder","metadata":{"id":"gtJfrEWghlF3"}},{"cell_type":"markdown","source":"The Decoder, is the expanding path of the U-Net Architecture.\n\nFrom the paper:\n\n*Every step in the expanding path consists of an upsampling of the feature map followed by a 2x2 convolution (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two 3x3 convolutions, each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution*\n\nNote that we have already implemented the part where two 3x3 convolutions occur followed by ReLU activation in `Block`. All we need to do to implement the `Decoder` is to add the “up-convolution” and the feature concatenation with correspondingly cropped feature map from the contracting path.\n\nIn PyTorch, the `ConvTranspose2d` operation performs the “up-convolution”. It accepts parameters like `in_channels`, `out_channels`, `kernel_size` and `stride` amongst others.\n\nSince the `in_channels` and `out_channels` values are different in the Decoder depending on where this operation is performed, in the implementation, the “up-convolution” operations are also stored as a list. Stride and kernel size are always 2 as mentioned in the paper.\n\nNow, all we need is to perform feature concatenation. Let’s look at the implementation of the Decoder to understand how all this works more clearly -","metadata":{"id":"Co58ImDNhlF3"}},{"cell_type":"code","source":"import torchvision\nclass Decoder(nn.Module):\n    def __init__(self, chs=(1024, 512, 256, 128, 64)):\n        super().__init__()\n        self.chs         = chs\n        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n\n    def forward(self, x, encoder_features):\n        for i in range(len(self.chs)-1):\n            x        = self.upconvs[i](x)\n            enc_ftrs = self.crop(encoder_features[i], x)\n            x        = torch.cat([x, enc_ftrs], dim=1)\n            x        = self.dec_blocks[i](x)\n        return x\n\n    def crop(self, enc_ftrs, x):\n        _, _, H, W = x.shape\n        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n        return enc_ftrs\n","metadata":{"id":"LQIjMyDihlF3","execution":{"iopub.status.busy":"2024-05-03T12:09:43.896066Z","iopub.execute_input":"2024-05-03T12:09:43.896701Z","iopub.status.idle":"2024-05-03T12:09:46.894543Z","shell.execute_reply.started":"2024-05-03T12:09:43.896661Z","shell.execute_reply":"2024-05-03T12:09:46.893539Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"So the `self.dec_blocks` is a list of `Decoder Blocks` that perform the two conv + ReLU operation as mentioned in the paper. The `self.upconvs` is a list of `ConvTranspose2d` operations that perform the “up-convolution” operations. And finally, in the `forward` function, the decoder accepts the `encoder_features` which were output by the Encoder to perform the concatenation operation before passing the result to `Block`.\n\nThat’s really all there is inside the `Decoder` of a U-Net. Let’s make sure this implementation works:","metadata":{"id":"hz23Qq34hlF3"}},{"cell_type":"code","source":"decoder = Decoder()\nx = torch.randn(1, 1024, 28, 28)\ndecoder(x, ftrs[::-1][1:]).shape","metadata":{"id":"mWkvQpwWhlF4","execution":{"iopub.status.busy":"2024-05-03T12:09:46.895686Z","iopub.execute_input":"2024-05-03T12:09:46.896089Z","iopub.status.idle":"2024-05-03T12:09:48.784395Z","shell.execute_reply.started":"2024-05-03T12:09:46.896063Z","shell.execute_reply":"2024-05-03T12:09:48.783409Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 64, 388, 388])"},"metadata":{}}]},{"cell_type":"markdown","source":"And there it is, the final feature map is of size `64x388x388` which matches that of `fig-1`. We have just successfully implemented both the Encoder and the Decoder so far.\n\nThe purpose of `ftrs[::-1][1:]` might be confusing.\n\nLet's recall the outputs of the Encoder. They were:\n```\ntorch.Size([1, 64, 568, 568]) #0\ntorch.Size([1, 128, 280, 280]) #1\ntorch.Size([1, 256, 136, 136]) #2\ntorch.Size([1, 512, 64, 64]) #3\ntorch.Size([1, 1024, 28, 28]) #4\n\n\n```","metadata":{"id":"zF_32jKuhlF4"}},{"cell_type":"markdown","source":"Now, from `fig-1`, we can see that the feature map with shape `torch.Size([1, 1024, 28, 28])` is never really concatenated but only a “up-convolution” operation is performed on it. Also, the 1st Decoder block in fig-1 accepts the inputs from the 3rd position Encoder block. Similarly, the 2nd Decoder block accepts the inputs from the 2nd position Encoder block and so on. Therefore, the encoder_features' order are reversed before passing them to the Decoder and since the feature map with shape `torch.Size([1, 1024, 28, 28])` is not concatenated to the Decoder blocks, it is not passed.\n\nHence, the input to the decoder is `ftrs[::-1][1:]`.","metadata":{"id":"yJf64r6LhlF4"}},{"cell_type":"markdown","source":"## U-Net\nGreat, we have so far implemented both the Encoder and the Decoder of U-Net architecture. Let’s put it all together to complete our implementation of U-Net.","metadata":{"id":"b1HkD5KUhlF4"}},{"cell_type":"code","source":"class UNet(nn.Module):\n    def __init__(self, enc_chs=(3,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=2, retain_dim=False, out_sz=(572,572)):\n        super().__init__()\n        self.encoder     = Encoder(enc_chs)\n        self.decoder     = Decoder(dec_chs)\n        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n        self.retain_dim  = retain_dim\n\n    def forward(self, x):\n        enc_ftrs = self.encoder(x)\n        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n        out      = self.head(out)\n        if self.retain_dim:\n            out = F.interpolate(out, out_sz)\n        return out","metadata":{"id":"h3u_BWkxhlF4","execution":{"iopub.status.busy":"2024-05-03T12:09:48.785531Z","iopub.execute_input":"2024-05-03T12:09:48.785844Z","iopub.status.idle":"2024-05-03T12:09:48.793319Z","shell.execute_reply.started":"2024-05-03T12:09:48.785818Z","shell.execute_reply":"2024-05-03T12:09:48.792189Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Let’s make sure this implementation works:","metadata":{"id":"pYz-N_4RhlF4"}},{"cell_type":"code","source":"unet = UNet()\nx    = torch.randn(1, 3, 572, 572)\nunet(x).shape\n","metadata":{"id":"LbM_AI_JhlF4","execution":{"iopub.status.busy":"2024-05-03T12:09:48.794558Z","iopub.execute_input":"2024-05-03T12:09:48.794875Z","iopub.status.idle":"2024-05-03T12:09:52.784020Z","shell.execute_reply.started":"2024-05-03T12:09:48.794848Z","shell.execute_reply":"2024-05-03T12:09:52.782839Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 2, 388, 388])"},"metadata":{}}]},{"cell_type":"markdown","source":"The output shape matches that of `fig-1`.","metadata":{"id":"96IGifVqhlF5"}},{"cell_type":"markdown","source":"# 3. Using U-net to do image segmentation","metadata":{"id":"ydqUZigzhlF5"}},{"cell_type":"markdown","source":"##  Data preparation","metadata":{"id":"vW87MM8ehlF5"}},{"cell_type":"markdown","source":"We will use a sample of the [Carvana](https://www.kaggle.com/c/carvana-image-masking-challenge) dataset to train the model to do image segmentation.\n\nLet’s talk about the data first. What we would like to have is 2 directories, something like `/Input` and `/Target`. In the `/Input` directory, we find all input images and in the `/Target` directory the segmentation maps. The labels are usually encoded with pixel values, meaning that all pixels of the same class have the same pixel value e.g. background=0, others=1","metadata":{"id":"_qWnPJlmhlF5"}},{"cell_type":"markdown","source":"Download the data in this [link](https://drive.google.com/drive/folders/1hh2QRzmyveps3C3GCDRkvRNsD5BslHIY?usp=sharing), upload to your google drive and mount to your colab","metadata":{"id":"QroFsWUIhlF5"}},{"cell_type":"code","source":"from google.colab import drive\n!mkdir drive\ndrive.mount('drive')","metadata":{"id":"v1LbX0fShlF5","execution":{"iopub.status.busy":"2024-05-03T12:09:52.785488Z","iopub.execute_input":"2024-05-03T12:09:52.785836Z","iopub.status.idle":"2024-05-03T12:09:53.067064Z","shell.execute_reply.started":"2024-05-03T12:09:52.785806Z","shell.execute_reply":"2024-05-03T12:09:53.064470Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmkdir drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"],"ename":"ModuleNotFoundError","evalue":"No module named 'google.colab'","output_type":"error"}]},{"cell_type":"markdown","source":"Then we list all the data in your drive to see if we mount successfully.","metadata":{"id":"Hd3n6X-ahlF5"}},{"cell_type":"code","source":"!ls \"drive/My Drive/\"","metadata":{"id":"-ZEcV_ovhlF6","execution":{"iopub.status.busy":"2024-05-03T12:09:53.068090Z","iopub.status.idle":"2024-05-03T12:09:53.068508Z","shell.execute_reply.started":"2024-05-03T12:09:53.068308Z","shell.execute_reply":"2024-05-03T12:09:53.068325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# modify the path according to your own file location\nimport pathlib\n\ndef get_filenames_of_path(path: pathlib.Path, ext: str = '*'):\n    \"\"\"Returns a list of files in a directory/path. Uses pathlib.\"\"\"\n    filenames = [str(file) for file in path.glob(ext) if file.is_file()]\n    # delete duplicates\n    filenames = [file for file in filenames if '(1)' not in file]\n    return sorted(filenames)\n\n# root = pathlib.Path.cwd() / 'Unet_data'\nroot = pathlib.Path(\"drive/My Drive/2AMM10/Unet_data\")\ninputs = get_filenames_of_path(root / 'Input')\ntargets = get_filenames_of_path(root / 'Target')","metadata":{"id":"uN993JlXhlF6","execution":{"iopub.status.busy":"2024-05-03T12:09:53.069967Z","iopub.status.idle":"2024-05-03T12:09:53.070367Z","shell.execute_reply.started":"2024-05-03T12:09:53.070170Z","shell.execute_reply":"2024-05-03T12:09:53.070186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let us split the data into training and validation dataset with the ratio 0.8","metadata":{"id":"FmXelM0qhlF6"}},{"cell_type":"code","source":"# random seed\nrandom_seed = 42\n\nfrom sklearn.model_selection import train_test_split\n# split dataset into training set and validation set\ntrain_size = 0.8  # 80:20 split\n\ninputs_train, inputs_valid = train_test_split(\n    inputs,\n    random_state=random_seed,\n    train_size=train_size,\n    shuffle=True)\n\ntargets_train, targets_valid = train_test_split(\n    targets,\n    random_state=random_seed,\n    train_size=train_size,\n    shuffle=True)","metadata":{"id":"8tRSrWNJhlF6","execution":{"iopub.status.busy":"2024-05-03T12:09:53.071689Z","iopub.status.idle":"2024-05-03T12:09:53.072103Z","shell.execute_reply.started":"2024-05-03T12:09:53.071925Z","shell.execute_reply":"2024-05-03T12:09:53.071942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can visualize some images from the training and validation dataset","metadata":{"id":"F3k6kbtIhlF6"}},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\n\n# visualize examples of training images and labels\ninput_ID=0\n\nprint (\"training image example\")\nx=Image.open(inputs_train[0])\nplt.imshow(x)\nplt.show()\n\nprint (\"training label example\")\nx=Image.open(targets_train[0])\nplt.imshow(x,cmap='binary')\nplt.show()\n\nprint (\"validation image example\")\nx=Image.open(inputs_valid[0])\nplt.imshow(x)\nplt.show()\n\nprint (\"validation label example\")\nx=Image.open(targets_valid[0])\nplt.imshow(x,cmap='binary')\nplt.show()","metadata":{"id":"ihTigFFthlF6","execution":{"iopub.status.busy":"2024-05-03T12:09:53.073602Z","iopub.status.idle":"2024-05-03T12:09:53.073999Z","shell.execute_reply.started":"2024-05-03T12:09:53.073818Z","shell.execute_reply":"2024-05-03T12:09:53.073835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the help of `Dataset`, we customized the training and validation dataset. The original's image resolution is too huge that may slow down the training process, so we use `transforms.Resize` to resize the dimension to `128x128`. `transforms.ToTensor()` could help us to transfer image data to torch tensors and normalize the values to `0~1`","metadata":{"id":"CwRY0HgzhlF7"}},{"cell_type":"code","source":"\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\n\nimport torchvision.transforms as transforms\nfrom torchvision.transforms import Compose, ToTensor, Resize\n\nimage_size=(128,128)\n\ndata_transform=transforms.Compose([\n                    transforms.Resize(image_size),\n                    transforms.ToTensor(),\n                    ])\n\n\nclass SegmentationDataSet(Dataset):\n    def __init__(self,\n                 inputs: list,\n                 targets: list,\n                 transform=None\n                 ):\n        self.inputs = inputs\n        self.targets = targets\n        self.transform = transform\n        self.inputs_dtype = torch.float32\n        self.targets_dtype = torch.long\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self,\n                    index: int):\n        # Select the sample\n        input_ID = self.inputs[index]\n        target_ID = self.targets[index]\n\n        # Load input and target\n        x=Image.open(input_ID)\n        y=Image.open(target_ID)\n\n        # Preprocessing\n        if self.transform is not None:\n            x=self.transform(x).type(self.inputs_dtype)\n            y=self.transform(y).type(self.targets_dtype)\n\n\n        return x, torch.squeeze(y)\n\n# dataset training\ndataset_train = SegmentationDataSet(inputs=inputs_train,\n                                    targets=targets_train,\n                                    transform=data_transform)\n\n# dataset validation\ndataset_valid = SegmentationDataSet(inputs=inputs_valid,\n                                    targets=targets_valid,\n                                    transform=data_transform)","metadata":{"id":"GSZxCoLxhlF7","execution":{"iopub.status.busy":"2024-05-03T12:09:53.075010Z","iopub.status.idle":"2024-05-03T12:09:53.075368Z","shell.execute_reply.started":"2024-05-03T12:09:53.075195Z","shell.execute_reply":"2024-05-03T12:09:53.075210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then we use `DataLoader` to create training and validation dataloader","metadata":{"id":"R77Km7bYhlF7"}},{"cell_type":"code","source":"\n\nfrom torch.utils.data import DataLoader\n\n# dataloader training\ndataloader_training = DataLoader(dataset=dataset_train,\n                                 batch_size=2,\n                                 shuffle=True)\n\n# dataloader validation\ndataloader_validation = DataLoader(dataset=dataset_valid,\n                                   batch_size=2,\n                                   shuffle=True)","metadata":{"id":"6St7McobhlF7","execution":{"iopub.status.busy":"2024-05-03T12:09:53.078539Z","iopub.status.idle":"2024-05-03T12:09:53.079102Z","shell.execute_reply.started":"2024-05-03T12:09:53.078833Z","shell.execute_reply":"2024-05-03T12:09:53.078856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Refactor the original U-Net","metadata":{"id":"jWGadJ1VhlF7"}},{"cell_type":"markdown","source":"The U-net we created in section 1 has depth 5, but it is not applicable in our case. Because we are now dealing with images with pixel `128x128x3`,  the output dimensions of the decoder block would be too small after several pooling layers. Try the following codes, and you will see an error like:","metadata":{"id":"84iK-FCshlF7"}},{"cell_type":"markdown","source":"```RuntimeError: Calculated padded input size per channel: (2 x 2). Kernel size: (3 x 3). Kernel size can't be greater than actual input size```\n","metadata":{"id":"chuOm7WOhlF7"}},{"cell_type":"code","source":"# Could raise error\n\nunet = UNet()\nx    = torch.randn(1, 3, 128, 128)\nunet(x).shape","metadata":{"id":"AC1LaDKChlF8","execution":{"iopub.status.busy":"2024-05-03T12:09:53.080184Z","iopub.status.idle":"2024-05-03T12:09:53.080695Z","shell.execute_reply.started":"2024-05-03T12:09:53.080439Z","shell.execute_reply":"2024-05-03T12:09:53.080461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Therefore, we need to reduce the depth, let's try depth=4","metadata":{"id":"tNxYMAC-hlF8"}},{"cell_type":"code","source":"enc_chs=(3,64,128,256,512)\ndec_chs=(512, 256, 128, 64)\n\nunet = UNet(enc_chs,dec_chs)\n\nx    = torch.randn(1, 3, 128, 128)\nunet(x).shape","metadata":{"id":"3DoQYVSThlF8","execution":{"iopub.status.busy":"2024-05-03T12:09:53.083597Z","iopub.status.idle":"2024-05-03T12:09:53.084020Z","shell.execute_reply.started":"2024-05-03T12:09:53.083835Z","shell.execute_reply":"2024-05-03T12:09:53.083853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We could also notice that in the original U-net, the output dimension is not the same as the input dimension. However, this is not the case with our data. Therefore, we need to use [`padding`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) in the `Block` to make sure outputs and inputs have the same dimensions.","metadata":{"id":"jzNzdhE2hlF8"}},{"cell_type":"code","source":"\nclass  Block(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n\n\n        # use padding to adjust the output dimension\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3,padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3,padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)","metadata":{"id":"ywk5AjzehlF8","execution":{"iopub.status.busy":"2024-05-03T12:09:53.085174Z","iopub.status.idle":"2024-05-03T12:09:53.085547Z","shell.execute_reply.started":"2024-05-03T12:09:53.085361Z","shell.execute_reply":"2024-05-03T12:09:53.085377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"enc_chs=(3,64,128,256,512)\ndec_chs=(512, 256, 128, 64)\n\nunet = UNet(enc_chs,dec_chs)\n\nx    = torch.randn(1, 3, 128, 128)\nunet(x).shape","metadata":{"id":"cRhc29ZAhlF8","execution":{"iopub.status.busy":"2024-05-03T12:09:53.086969Z","iopub.status.idle":"2024-05-03T12:09:53.087334Z","shell.execute_reply.started":"2024-05-03T12:09:53.087154Z","shell.execute_reply":"2024-05-03T12:09:53.087169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, the outputs have the same dimention with the inputs,`128x128`","metadata":{"id":"jJibG8JxhlF8"}},{"cell_type":"markdown","source":"## Train the model","metadata":{"id":"_ni_9UmVhlF8"}},{"cell_type":"markdown","source":"Next let's train the model. First, we creat some the `Trainer` class","metadata":{"id":"6Zlu17KvhlF9"}},{"cell_type":"code","source":"import numpy as np\nfrom tqdm import tqdm\n\n\nclass Trainer():\n    def __init__(self,\n                 model: torch.nn.Module,\n                 device: torch.device,\n                 criterion: torch.nn.Module,\n                 optimizer: torch.optim.Optimizer,\n                 training_DataLoader: torch.utils.data.Dataset,\n                 validation_DataLoader: torch.utils.data.Dataset ,\n                 epochs: int\n                 ):\n\n        self.model = model\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.training_DataLoader = training_DataLoader\n        self.validation_DataLoader = validation_DataLoader\n        self.device = device\n        self.epochs = epochs\n\n\n\n\n    def run_trainer(self):\n\n\n        for epoch in tqdm(range(self.epochs)):\n\n\n\n            self.model.train()  # train mode\n\n            train_losses=[]\n            for batch in self.training_DataLoader:\n\n                x,y=batch\n                input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n                self.optimizer.zero_grad()  # zerograd the parameters\n                out = self.model(input)  # one forward pass\n                loss = self.criterion(out, target)  # calculate loss\n\n                loss_value = loss.item()\n                train_losses.append(loss_value)\n\n                loss.backward()  # one backward pass\n                self.optimizer.step()  # update the parameters\n\n\n\n            self.model.eval()  # evaluation mode\n            valid_losses = []  # accumulate the losses here\n\n            for batch in self.validation_DataLoader:\n\n                x,y=batch\n                input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n\n                with torch.no_grad():\n                    out = self.model(input)   # one forward pass\n                    loss = self.criterion(out, target) # calculate loss\n\n                    loss_value = loss.item()\n                    valid_losses.append(loss_value)\n\n\n\n            # print the results\n            print(\n                f'EPOCH: {epoch+1:0>{len(str(self.epochs))}}/{self.epochs}',\n                end=' '\n            )\n            print(f'LOSS: {np.mean(train_losses):.4f}',end=' ')\n            print(f'VAL-LOSS: {np.mean(valid_losses):.4f}',end='\\n')\n\n\n","metadata":{"id":"2AWqzVW4hlF9","execution":{"iopub.status.busy":"2024-05-03T12:09:53.088805Z","iopub.status.idle":"2024-05-03T12:09:53.089194Z","shell.execute_reply.started":"2024-05-03T12:09:53.089009Z","shell.execute_reply":"2024-05-03T12:09:53.089026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's train the model, don't forget to move your model to GPU . Loss value should decrease along with epochs","metadata":{"id":"3l19qzsIhlF9"}},{"cell_type":"code","source":"\n\n# device\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device=torch.device('cpu')\n\n\n\nmodel=unet.to(device)\n\n# criterion\ncriterion = torch.nn.CrossEntropyLoss()\n\n# optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# trainer\ntrainer = Trainer(model=model,\n                  device=device,\n                  criterion=criterion,\n                  optimizer=optimizer,\n                  training_DataLoader=dataloader_training,\n                  validation_DataLoader=dataloader_validation,\n                  epochs=4)\n\n# start training\ntrainer.run_trainer()","metadata":{"id":"C-Aklv-jhlF9","execution":{"iopub.status.busy":"2024-05-03T12:09:53.090647Z","iopub.status.idle":"2024-05-03T12:09:53.091034Z","shell.execute_reply.started":"2024-05-03T12:09:53.090850Z","shell.execute_reply":"2024-05-03T12:09:53.090865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot the segmentation","metadata":{"id":"yNMCAtHPhlF9"}},{"cell_type":"markdown","source":"We have three test images apart from the training and validation dataset, lets use the trained model to make predictions on these test images.","metadata":{"id":"18O51RvRhlF9"}},{"cell_type":"code","source":"# test input and  test target files\n# modify the path according to your own file location\n\nTest_root = root / 'Test'\nimages_names = get_filenames_of_path(Test_root / 'Input')\ntargets_names = get_filenames_of_path(Test_root / 'Target')","metadata":{"id":"L9-L69KGhlF-","execution":{"iopub.status.busy":"2024-05-03T12:09:53.092824Z","iopub.status.idle":"2024-05-03T12:09:53.093190Z","shell.execute_reply.started":"2024-05-03T12:09:53.093017Z","shell.execute_reply":"2024-05-03T12:09:53.093032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nfor index in range(len(images_names)):\n    print (\"\\n\",\"image index\",index)\n    # Show original image\n    ori_image=Image.open(images_names[index])\n    print (\"original image\")\n    ori_image=data_transform(ori_image)\n    t_ori_image=ori_image.permute(1,2,0)\n\n    plt.imshow(t_ori_image)\n    plt.show()\n\n\n    # Show label image\n    label_img=Image.open(targets_names[index])\n    print (\"label image\")\n    label_img=data_transform(label_img)\n    label_img=torch.squeeze(label_img)# squeeze the data\n    plt.imshow(label_img,cmap='binary')\n    plt.show()\n\n\n    # make predition and show prediction image\n    x=torch.unsqueeze(ori_image,0)# add batch dimension [B, C, H, W]\n\n\n    model.eval()\n    with torch.no_grad():\n        out = model(x.to(device))  # send through model/network\n    out_softmax = torch.softmax(out, dim=1)  # perform softmax on outputs\n\n    img = torch.argmax(out_softmax, dim=1)  # perform argmax to generate 1 channel\n    img = img.cpu().numpy()  # send to cpu and transform to numpy.ndarray\n    img = np.squeeze(img)  # remove batch dim and channel dim -> [H, W]\n    print (\"prediction\")\n    plt.imshow(img,cmap='binary')\n    plt.show()","metadata":{"id":"eNbFyLwwhlF-","execution":{"iopub.status.busy":"2024-05-03T12:09:53.094655Z","iopub.status.idle":"2024-05-03T12:09:53.095025Z","shell.execute_reply.started":"2024-05-03T12:09:53.094851Z","shell.execute_reply":"2024-05-03T12:09:53.094867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"references:\n\n1) https://towardsdatascience.com/creating-and-training-a-u-net-model-with-pytorch-for-2d-3d-semantic-segmentation-model-building-6ab09d6a0862\n\n2) https://amaarora.github.io/2020/09/13/unet.html\n\n3) https://github.com/milesial/Pytorch-UNet","metadata":{"id":"HbhXzvvbhlF-"}}]}