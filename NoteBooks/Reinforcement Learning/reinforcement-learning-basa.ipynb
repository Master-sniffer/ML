{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Simple guide for RF in python. Just use Copy this notebook to your local pc and run all to see how it works !","metadata":{}},{"cell_type":"code","source":"%pip install stable-baselines3[extra] -q\n%pip install gym[all] -q\n%pip install tensorboard -q","metadata":{},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import gym, os\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.evaluation import evaluate_policy","metadata":{},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"environment_name = \"CartPole-v0\"\nenv = gym.make(environment_name)\n","metadata":{},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\nepisodes = 5 # Testing our env 5 Times ! (You can think of 1 episode as one full game)\nfor episode in range(1,episodes+1):\n    state = env.reset() # This one sets an initial env. It generates observations so to understand best type of action and reward to it\n    done = False # Is episode doen\n    score = 0 \n    \n    while not done:\n        env.render() # This allows us to view env in graphics\n        action = env.action_space.sample() #Here we generate a random action\n        n_state, reward, done, info = env.step(action) # Env.step for action with the env \n        action += reward\n    print('Episode: {} Score{}'.format(episode,score))\nenv.close() # Close render screen (graphics screen)","metadata":{},"execution_count":6,"outputs":[{"name":"stdout","output_type":"stream","text":"Episode: 1 Score0\n\nEpisode: 2 Score0\n\nEpisode: 3 Score0\n\nEpisode: 4 Score0\n\nEpisode: 5 Score0\n"}]},{"cell_type":"markdown","source":"# See the Env","metadata":{}},{"cell_type":"code","source":"env.action_space","metadata":{},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":["Discrete(2)"]},"metadata":{}}]},{"cell_type":"code","source":"env.observation_space.sample() # We get 4 values, which mean CART POSITION, CART VELOCITY, POLE ANGLE, POLE ANGULAR VELOCITY","metadata":{},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":["array([ 2.3256021e+00, -1.9713159e+38,  2.9394504e-01, -8.2985874e+37],\n","      dtype=float32)"]},"metadata":{}}]},{"cell_type":"markdown","source":"# Train agent","metadata":{}},{"cell_type":"code","source":"log_path = os.path.join('Training','Logs')","metadata":{},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"environment_name = \"CartPole-v0\"\nenv = gym.make(environment_name) # Created here our env\nenv = DummyVecEnv([lambda:env]) # This will let us work in an env being covered by DummyVec(aka wrap)\nmodel = PPO('MlpPolicy',env,verbose=1, tensorboard_log=log_path) #Define the agent","metadata":{},"execution_count":7,"outputs":[{"name":"stdout","output_type":"stream","text":"Using cpu device\n"}]},{"cell_type":"markdown","source":"policy = this is agent's policy and to understand it do it like that: think of that policy as the rule which tells how to operate in the env.","metadata":{}},{"cell_type":"code","source":"model.learn(total_timesteps=20_000) # How long we want to train","metadata":{},"execution_count":27,"outputs":[{"name":"stdout","output_type":"stream","text":"Logging to Training\\Logs\\PPO_1\n\n-----------------------------\n\n| time/              |      |\n\n|    fps             | 1172 |\n\n|    iterations      | 1    |\n\n|    time_elapsed    | 1    |\n\n|    total_timesteps | 2048 |\n\n-----------------------------\n\n-----------------------------------------\n\n| time/                   |             |\n\n|    fps                  | 824         |\n\n|    iterations           | 2           |\n\n|    time_elapsed         | 4           |\n\n|    total_timesteps      | 4096        |\n\n| train/                  |             |\n\n|    approx_kl            | 0.009386983 |\n\n|    clip_fraction        | 0.122       |\n\n|    clip_range           | 0.2         |\n\n|    entropy_loss         | -0.686      |\n\n|    explained_variance   | -0.00359    |\n\n|    learning_rate        | 0.0003      |\n\n|    loss                 | 10.8        |\n\n|    n_updates            | 10          |\n\n|    policy_gradient_loss | -0.0184     |\n\n|    value_loss           | 60.4        |\n\n-----------------------------------------\n\n-----------------------------------------\n\n| time/                   |             |\n\n|    fps                  | 764         |\n\n|    iterations           | 3           |\n\n|    time_elapsed         | 8           |\n\n|    total_timesteps      | 6144        |\n\n| train/                  |             |\n\n|    approx_kl            | 0.008843591 |\n\n|    clip_fraction        | 0.0546      |\n\n|    clip_range           | 0.2         |\n\n|    entropy_loss         | -0.667      |\n\n|    explained_variance   | 0.0629      |\n\n|    learning_rate        | 0.0003      |\n\n|    loss                 | 16.2        |\n\n|    n_updates            | 20          |\n\n|    policy_gradient_loss | -0.0159     |\n\n|    value_loss           | 38.3        |\n\n-----------------------------------------\n\n-----------------------------------------\n\n| time/                   |             |\n\n|    fps                  | 754         |\n\n|    iterations           | 4           |\n\n|    time_elapsed         | 10          |\n\n|    total_timesteps      | 8192        |\n\n| train/                  |             |\n\n|    approx_kl            | 0.007860271 |\n\n|    clip_fraction        | 0.0816      |\n\n|    clip_range           | 0.2         |\n\n|    entropy_loss         | -0.641      |\n\n|    explained_variance   | 0.242       |\n\n|    learning_rate        | 0.0003      |\n\n|    loss                 | 24.5        |\n\n|    n_updates            | 30          |\n\n|    policy_gradient_loss | -0.0177     |\n\n|    value_loss           | 54.9        |\n\n-----------------------------------------\n\n-----------------------------------------\n\n| time/                   |             |\n\n|    fps                  | 740         |\n\n|    iterations           | 5           |\n\n|    time_elapsed         | 13          |\n\n|    total_timesteps      | 10240       |\n\n| train/                  |             |\n\n|    approx_kl            | 0.008690704 |\n\n|    clip_fraction        | 0.0643      |\n\n|    clip_range           | 0.2         |\n\n|    entropy_loss         | -0.614      |\n\n|    explained_variance   | 0.219       |\n\n|    learning_rate        | 0.0003      |\n\n|    loss                 | 33.9        |\n\n|    n_updates            | 40          |\n\n|    policy_gradient_loss | -0.0167     |\n\n|    value_loss           | 65.5        |\n\n-----------------------------------------\n\n----------------------------------------\n\n| time/                   |            |\n\n|    fps                  | 724        |\n\n|    iterations           | 6          |\n\n|    time_elapsed         | 16         |\n\n|    total_timesteps      | 12288      |\n\n| train/                  |            |\n\n|    approx_kl            | 0.00600616 |\n\n|    clip_fraction        | 0.0582     |\n\n|    clip_range           | 0.2        |\n\n|    entropy_loss         | -0.589     |\n\n|    explained_variance   | 0.397      |\n\n|    learning_rate        | 0.0003     |\n\n|    loss                 | 18         |\n\n|    n_updates            | 50         |\n\n|    policy_gradient_loss | -0.0138    |\n\n|    value_loss           | 64.7       |\n\n----------------------------------------\n\n-----------------------------------------\n\n| time/                   |             |\n\n|    fps                  | 712         |\n\n|    iterations           | 7           |\n\n|    time_elapsed         | 20          |\n\n|    total_timesteps      | 14336       |\n\n| train/                  |             |\n\n|    approx_kl            | 0.006297054 |\n\n|    clip_fraction        | 0.0421      |\n\n|    clip_range           | 0.2         |\n\n|    entropy_loss         | -0.59       |\n\n|    explained_variance   | 0.692       |\n\n|    learning_rate        | 0.0003      |\n\n|    loss                 | 4.95        |\n\n|    n_updates            | 60          |\n\n|    policy_gradient_loss | -0.00697    |\n\n|    value_loss           | 38.3        |\n\n-----------------------------------------\n\n------------------------------------------\n\n| time/                   |              |\n\n|    fps                  | 706          |\n\n|    iterations           | 8            |\n\n|    time_elapsed         | 23           |\n\n|    total_timesteps      | 16384        |\n\n| train/                  |              |\n\n|    approx_kl            | 0.0054771313 |\n\n|    clip_fraction        | 0.0508       |\n\n|    clip_range           | 0.2          |\n\n|    entropy_loss         | -0.574       |\n\n|    explained_variance   | 0.779        |\n\n|    learning_rate        | 0.0003       |\n\n|    loss                 | 12.1         |\n\n|    n_updates            | 70           |\n\n|    policy_gradient_loss | -0.00873     |\n\n|    value_loss           | 34.6         |\n\n------------------------------------------\n\n-----------------------------------------\n\n| time/                   |             |\n\n|    fps                  | 699         |\n\n|    iterations           | 9           |\n\n|    time_elapsed         | 26          |\n\n|    total_timesteps      | 18432       |\n\n| train/                  |             |\n\n|    approx_kl            | 0.008925892 |\n\n|    clip_fraction        | 0.0931      |\n\n|    clip_range           | 0.2         |\n\n|    entropy_loss         | -0.575      |\n\n|    explained_variance   | 0.851       |\n\n|    learning_rate        | 0.0003      |\n\n|    loss                 | 2.36        |\n\n|    n_updates            | 80          |\n\n|    policy_gradient_loss | -0.0116     |\n\n|    value_loss           | 26.9        |\n\n-----------------------------------------\n\n-----------------------------------------\n\n| time/                   |             |\n\n|    fps                  | 698         |\n\n|    iterations           | 10          |\n\n|    time_elapsed         | 29          |\n\n|    total_timesteps      | 20480       |\n\n| train/                  |             |\n\n|    approx_kl            | 0.007843136 |\n\n|    clip_fraction        | 0.0828      |\n\n|    clip_range           | 0.2         |\n\n|    entropy_loss         | -0.589      |\n\n|    explained_variance   | 0.915       |\n\n|    learning_rate        | 0.0003      |\n\n|    loss                 | 1.03        |\n\n|    n_updates            | 90          |\n\n|    policy_gradient_loss | -0.0119     |\n\n|    value_loss           | 13.1        |\n\n-----------------------------------------\n"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":["<stable_baselines3.ppo.ppo.PPO at 0x1dfbee88850>"]},"metadata":{}}]},{"cell_type":"code","source":"PPO_Path = os.path.join('Training', 'Saved Models', 'PPO_model')","metadata":{},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model.save(PPO_Path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model","metadata":{},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = PPO.load(PPO_Path, env=env)","metadata":{},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Evaluation ","metadata":{}},{"cell_type":"code","source":"evaluate_policy(model, env, n_eval_episodes=10, render=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.close()","metadata":{},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"\nepisodes = 5 # Testing our env 5 Times ! (You can think of 1 episode as one full game)\nfor episode in range(1,episodes+1):\n    obs = env.reset() # AGAIN, HERE WE GET OBSERVATIONS FOR OBSERVATION SPACE\n    done = False # Is episode doen\n    score = 0 \n    \n    while not done:\n        env.render() # This allows us to view env in graphics\n        action, _states = model.predict(obs) # Instead of samples we will use predictions\n        obs, reward, done, info = env.step(action) # Env.step for action with the env \n        score += reward\n    print('Episode: {} Score{}'.format(episode,score))\nenv.close() # Close render screen (graphics screen)","metadata":{},"execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":"Episode: 1 Score200.0\n\nEpisode: 2 Score200.0\n\nEpisode: 3 Score200.0\n\nEpisode: 4 Score200.0\n\nEpisode: 5 Score200.0\n"}]},{"cell_type":"code","source":"obs = env.reset()","metadata":{},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"obs # THESE FOUR DESCRIBE US THESE: Cart position, Cart Velocity, Pole angle, Pole Angular Velocity","metadata":{},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":["array([[-0.00671012,  0.01133617,  0.03237586,  0.00465344]],\n","      dtype=float32)"]},"metadata":{}}]},{"cell_type":"code","source":"model.predict(obs)","metadata":{},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":["(array([0], dtype=int64), None)"]},"metadata":{}}]},{"cell_type":"code","source":"action , _ = model.predict(obs)","metadata":{},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"env.step(action) # Here reward is 1 or 0. We get 0 if our poll falls, otherwise 1","metadata":{},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":["(array([[-0.0064834 ,  0.2059792 ,  0.03246893, -0.27764127]],\n","       dtype=float32),\n"," array([1.], dtype=float32),\n"," array([False]),\n"," [{}])"]},"metadata":{}}]},{"cell_type":"markdown","source":"# Using Tensorboard","metadata":{}},{"cell_type":"code","source":"training_log_path = os.path.join(log_path, 'PPO_1')","metadata":{},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"!tensorboard --logdir={training_log_path}\n","metadata":{},"execution_count":20,"outputs":[{"name":"stdout","output_type":"stream","text":"^C\n"}]},{"cell_type":"markdown","source":"# Adding a callback to training","metadata":{}},{"cell_type":"code","source":"from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\nimport os","metadata":{},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"save_path = os.path.join('Training', 'Saved Models')\nlog_path = os.path.join('Training', 'Logs')","metadata":{},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"stop_callback = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)\neval_callback = EvalCallback(env, callback_on_new_best=stop_callback,\n                            eval_freq=10000,\n                            best_model_save_path= save_path,\n                            verbose=1)","metadata":{},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"model = PPO('MlpPolicy', env,verbose=1,tensorboard_log=log_path)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.learn(total_timesteps=20_000,callback=eval_callback)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Changing Policy","metadata":{}},{"cell_type":"code","source":"new_arch = [dict(pi=[128,128,128,128], vf=[128,128,128,128])] # This is our new architecture of our NN","metadata":{},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model = PPO('MlpPolicy', env,verbose=1,tensorboard_log=log_path , policy_kwargs={'net_arch':new_arch}) # To specify NEW neural network and NEW policy we use policy_kwargs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.learn(total_timesteps=20_000,callback=eval_callback)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using ALT Algorithm","metadata":{}},{"cell_type":"code","source":"from stable_baselines3 import DQN\n","metadata":{},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model = DQN('MlpPolicy', env,verbose=1,tensorboard_log=log_path) ","metadata":{},"execution_count":21,"outputs":[{"name":"stdout","output_type":"stream","text":"Using cpu device\n"}]},{"cell_type":"code","source":"model.learn(total_timesteps=20_000,callback=eval_callback)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}