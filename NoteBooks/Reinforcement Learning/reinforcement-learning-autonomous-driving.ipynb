{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mastersniffer/reinforcement-learning-autonomous-driving?scriptVersionId=128824828\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# WARNING: ONLY ON LOCAL MACHINE\n\nI've tried to launch in kaggle (even tried without render), but it didn't work. If you DO know how to not render or how to display everything - Please write a comment","metadata":{}},{"cell_type":"code","source":"%pip install pyglet -q\n%pip install stable_baselines3 -q\n%pip install swig -q\n%pip install box2d -q\n%pip install box2d-py -q\n%pip install gym -q\n%pip install gym[box2d] -q\n%pip install mujoco_py -q\n%pip install MuJoCo -q\n%pip install gym[all]\n%pip install gym[CarRacing]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gym\nimport importlib\nimportlib.reload(gym)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import VecFrameStack\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.evaluation import evaluate_policy\nimport os","metadata":{},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"Don't react on any errors if you see them...\n\nJust follow the path\n\n![Alt Text](https://gifdb.com/images/file/chill-zen-patrick-spongebob-ta7u9ezys7t6ahgv.gif)","metadata":{}},{"cell_type":"code","source":"environment_name = \"CarRacing-v0\"\nenv = gym.make(environment_name)","metadata":{},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"episodes = 5\nfor episode in range(1, episodes+1):\n    state = env.reset()\n    done = False\n    score = 0 \n    \n    while not done:\n        env.render()\n        action = env.action_space.sample()\n        n_state, reward, done, info = env.step(action)\n        score+=reward\n    print('Episode:{} Score:{}'.format(episode, score))\nenv.close()","metadata":{},"execution_count":29,"outputs":[{"name":"stdout","output_type":"stream","text":"Track generation: 1162..1457 -> 295-tiles track\n\nEpisode:1 Score:-31.97278911564669\n\nTrack generation: 1040..1304 -> 264-tiles track\n\nEpisode:2 Score:-27.756653992395773\n\nTrack generation: 1122..1407 -> 285-tiles track\n\nEpisode:3 Score:-29.577464788732794\n\nTrack generation: 1004..1263 -> 259-tiles track\n\nretry to generate track (normal if there are not manyinstances of this message)\n\nTrack generation: 1162..1456 -> 294-tiles track\n\nEpisode:4 Score:-31.740614334471474\n\nTrack generation: 1204..1509 -> 305-tiles track\n\nEpisode:5 Score:-34.21052631579003\n"}]},{"cell_type":"markdown","source":"# Training Model","metadata":{}},{"cell_type":"code","source":"env = gym.make(environment_name)\nenv = DummyVecEnv([lambda:env])","metadata":{},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"log_path = os.path.join('Training', 'Logs')\nmodel = PPO('CnnPolicy', env, verbose=1,tensorboard_log=log_path)","metadata":{},"execution_count":56,"outputs":[{"name":"stdout","output_type":"stream","text":"Using cpu device\n\nWrapping the env in a VecTransposeImage.\n"}]},{"cell_type":"code","source":"model.learn(total_timesteps=1_000)","metadata":{},"execution_count":57,"outputs":[{"name":"stdout","output_type":"stream","text":"Track generation: 1096..1374 -> 278-tiles track\n\nLogging to Training\\Logs\\PPO_8\n\nTrack generation: 1307..1638 -> 331-tiles track\n\nTrack generation: 1220..1529 -> 309-tiles track\n\n-----------------------------\n\n| time/              |      |\n\n|    fps             | 128  |\n\n|    iterations      | 1    |\n\n|    time_elapsed    | 15   |\n\n|    total_timesteps | 2048 |\n\n-----------------------------\n"},{"execution_count":57,"output_type":"execute_result","data":{"text/plain":["<stable_baselines3.ppo.ppo.PPO at 0x2555c385a00>"]},"metadata":{}}]},{"cell_type":"code","source":"env.close()","metadata":{},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"ppo_path = os.path.join('Training', 'Saved Models', 'PPO_Driving_model')","metadata":{},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"model.save(ppo_path)","metadata":{},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"# TRYING TO SOLVE THE PROBLEM THAT IT DOESNT SEE\n#del model \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model = PPO(ppo_path, env)","metadata":{},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"# Evaluating","metadata":{}},{"cell_type":"code","source":"evaluate_policy(model,env,n_eval_episodes=10, render=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To see the model perfomance you can look at numbers up here (which are written in tuple)\n\nExample:\n\n(-38.48090168684721, 20.011292318268367)\n\nHere -38 means average score and 20 means standart diviation (in other words how far from mean value the scores are scattered)\n","metadata":{}},{"cell_type":"code","source":"env.close()","metadata":{},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"episodes = 5\nfor episode in range(1, episodes+1):\n    obs = env.reset()\n    done = False\n    score = 0 \n    \n    while not done:\n        env.render()\n        action,_ = model.predict(obs)\n        obs, reward, done, info = env.step(action)\n        score+=reward\n    print('Episode:{} Score:{}'.format(episode, score))\nenv.close()","metadata":{},"execution_count":69,"outputs":[{"name":"stdout","output_type":"stream","text":"Track generation: 1159..1456 -> 297-tiles track\n\nTrack generation: 1057..1325 -> 268-tiles track\n\nEpisode:1 Score:[-52.70226]\n\nTrack generation: 1197..1501 -> 304-tiles track\n\nTrack generation: 1415..1773 -> 358-tiles track\n\nEpisode:2 Score:[-53.794975]\n\nTrack generation: 1190..1491 -> 301-tiles track\n\nTrack generation: 1155..1448 -> 293-tiles track\n\nEpisode:3 Score:[-49.99965]\n\nTrack generation: 1117..1410 -> 293-tiles track\n\nTrack generation: 1235..1548 -> 313-tiles track\n\nEpisode:4 Score:[-55.47907]\n\nTrack generation: 1292..1619 -> 327-tiles track\n\nTrack generation: 1319..1653 -> 334-tiles track\n\nEpisode:5 Score:[-63.189747]\n"}]},{"cell_type":"code","source":"env.close()","metadata":{},"execution_count":66,"outputs":[]}]}